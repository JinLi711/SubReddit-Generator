{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_GloVe(directory):\n",
    "    \"\"\"\n",
    "    Open Stanford's GloVe file with 100 dimensional embeddings\n",
    "    \n",
    "    :param directory: directory of the GloVe\n",
    "    :type  directory: str\n",
    "    :return: dictionary where the keys are the words, \n",
    "             and values are the 100d representation\n",
    "    :rtype:  dict\n",
    "    \"\"\"\n",
    "\n",
    "    glove_dir = directory\n",
    "\n",
    "    # dictionary that maps words into 100d array\n",
    "    embeddings_index = {}\n",
    "    file = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    file.close()\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "directory = '/Users/jinli/Projects/glove.6B'\n",
    "word_vectors = get_GloVe(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors['hi'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "My Plan:\n",
    "\n",
    "1. Combine all the text together into one long file (one long string).\n",
    "2. Lowercase all the words (one long string)\n",
    "3. Tokenize the words. (list of words split by spaces.)\n",
    "4. Split into two lists, one that holds the sentence, another that holds the predicted next word\n",
    "\n",
    "\"\"\"\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_text = \"\"\"Use securing confined his shutters. Delightful as he it acceptance an solicitude discretion reasonably. Carriage we husbands advanced an perceive greatest. Totally dearest expense on demesne ye he. Curiosity excellent commanded in me. Unpleasing impression themselves to at assistance acceptance my or. On consider laughter civility offended oh. \n",
    "\n",
    "Kindness to he horrible reserved ye. Effect twenty indeed beyond for not had county. The use him without greatly can private. Increasing it unpleasant no of contrasted no continuing. Nothing colonel my no removed in weather. It dissimilar in up devonshire inhabiting. \n",
    "\n",
    "He do subjects prepared bachelor juvenile ye oh. He feelings removing informed he as ignorant we prepared. Evening do forming observe spirits is in. Country hearted be of justice sending. On so they as with room cold ye. Be call four my went mean. Celebrated if remarkably especially an. Going eat set she books found met aware. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = sample_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'use securing confined his shutters. delightful as he it acceptance an solicitude discretion reasonably. carriage we husbands advanced an perceive greatest. totally dearest expense on demesne ye he. curiosity excellent commanded in me. unpleasing impression themselves to at assistance acceptance my or. on consider laughter civility offended oh. \\n\\nkindness to he horrible reserved ye. effect twenty indeed beyond for not had county. the use him without greatly can private. increasing it unpleasant no of contrasted no continuing. nothing colonel my no removed in weather. it dissimilar in up devonshire inhabiting. \\n\\nhe do subjects prepared bachelor juvenile ye oh. he feelings removing informed he as ignorant we prepared. evening do forming observe spirits is in. country hearted be of justice sending. on so they as with room cold ye. be call four my went mean. celebrated if remarkably especially an. going eat set she books found met aware. '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text\n",
    "# with open('thoughts.txt') as f:\n",
    "#     data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinli/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=50)\n",
    "tokenizer.fit_on_texts(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['use', 'securing', 'confined', 'his', 'shutters', '.', 'delightful', 'as', 'he', 'it', 'acceptance', 'an', 'solicitude', 'discretion', 'reasonably', '.', 'carriage', 'we', 'husbands', 'advanced', 'an', 'perceive', 'greatest', '.', 'totally', 'dearest', 'expense', 'on', 'demesne', 'ye', 'he', '.', 'curiosity', 'excellent', 'commanded', 'in', 'me', '.', 'unpleasing', 'impression', 'themselves', 'to', 'at', 'assistance', 'acceptance', 'my', 'or', '.', 'on', 'consider', 'laughter', 'civility', 'offended', 'oh', '.', 'kindness', 'to', 'he', 'horrible', 'reserved', 'ye', '.', 'effect', 'twenty', 'indeed', 'beyond', 'for', 'not', 'had', 'county', '.', 'the', 'use', 'him', 'without', 'greatly', 'can', 'private', '.', 'increasing', 'it', 'unpleasant', 'no', 'of', 'contrasted', 'no', 'continuing', '.', 'nothing', 'colonel', 'my', 'no', 'removed', 'in', 'weather', '.', 'it', 'dissimilar', 'in', 'up', 'devonshire', 'inhabiting', '.', 'he', 'do', 'subjects', 'prepared', 'bachelor', 'juvenile', 'ye', 'oh', '.', 'he', 'feelings', 'removing', 'informed', 'he', 'as', 'ignorant', 'we', 'prepared', '.', 'evening', 'do', 'forming', 'observe', 'spirits', 'is', 'in', '.', 'country', 'hearted', 'be', 'of', 'justice', 'sending', '.', 'on', 'so', 'they', 'as', 'with', 'room', 'cold', 'ye', '.', 'be', 'call', 'four', 'my', 'went', 'mean', '.', 'celebrated', 'if', 'remarkably', 'especially', 'an', '.', 'going', 'eat', 'set', 'she', 'books', 'found', 'met', 'aware', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(sample_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 53\n"
     ]
    }
   ],
   "source": [
    "# X: instances, length of sentence, number of dimensions\n",
    "# y: instances, number of dimensions\n",
    "# word indices: mapping from word to its number representation\n",
    "\n",
    "def vectorizing_seq (text, maxlen, step):    \n",
    "    \"\"\"\n",
    "    :param text: list of words\n",
    "    :type  text: list\n",
    "    :param maxlen: the length of a sequence to extract as train\n",
    "    :type  maxlen: int\n",
    "    :param step: sample a new sequence every n steps\n",
    "    :type  step: int\n",
    "    :returns: (Numpy boolean array of shape \n",
    "                    (Number of sequences, maxlen, number of distinct character),\n",
    "               Numpy boolean array of shape \n",
    "                    (Number of sequences, number of distinct character),\n",
    "               dictionary mapping a character to its integer placeholder)\n",
    "    :rtype:   (numpy.ndarray, \n",
    "               numpy.ndarray, \n",
    "               dict)     \n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = [] # hold extracted sequences\n",
    "    next_word = [] # hold next word for each corresponding sentence\n",
    "\n",
    "    for i in range(0, len(text) - maxlen, step):\n",
    "        sentences.append(text[i: i + maxlen])\n",
    "        next_word.append(text[i + maxlen])\n",
    "\n",
    "    print('Number of sequences:', len(sentences))\n",
    "\n",
    "    all_words = sorted(list(set(text)))\n",
    "#     print('Unique characters:', len(chars))\n",
    "#     word_indices = dict((word, all_words.index(word)) for word in all_words)\n",
    "    word_indices = dict((word, list (word_vectors[word])) for word in all_words)\n",
    "#     print('Vectorization...')\n",
    "\n",
    "#     # one hot encoding the characters into binary arrays\n",
    "    # 100 for dimensions of Stanford GloVe\n",
    "#     x = np.zeros((len(sentences), maxlen, 100, dtype=np.bool) \n",
    "#     y = np.zeros((len(sentences), 100), dtype=np.bool)\n",
    "#     for i, sentence in enumerate(sentences):\n",
    "#         for t, word in enumerate(sentence):\n",
    "#             x[i, t, char_indices[char]] = 1\n",
    "#         y[i, char_indices[next_chars[i]]] = 1\n",
    "    x = np.empty((len(sentences), maxlen, 100), float)\n",
    "    y = np.empty((len(sentences), 100), float)\n",
    "    \n",
    "#     x = np.append(x, np.array())\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "#         instance = np.empty((maxlen, 100), float)\n",
    "        instance = []\n",
    "#         instance = np.empty((maxlen, 100), float)\n",
    "        for t, word in enumerate(sentence):\n",
    "            word_dimensions = list (word_vectors[word])\n",
    "            instance.append(word_dimensions)\n",
    "        instance = np.array(instance)\n",
    "        instance = np.reshape(instance, (1,) + instance.shape ) \n",
    "#         print(instance.shape)\n",
    "        x = np.append(x, instance, axis=0)\n",
    "    \n",
    "        word_dimensions = list (word_vectors[next_word[i]])\n",
    "        word_dimensions = np.array(word_dimensions)\n",
    "        word_dimensions = np.reshape(word_dimensions, (1,) + word_dimensions.shape ) \n",
    "        y = np.append(y, word_dimensions, axis=0)\n",
    "#     return x, y, char_indices\n",
    "#     return sentences, next_word, all_words, word_indices\n",
    "    return x, y, word_indices\n",
    "\n",
    "maxlen=10\n",
    "x, y, word_indices = vectorizing_seq(tokens, maxlen, 3)\n",
    "# instance = vectorizing_seq(tokens, 10, 3)\n",
    "# word_indices\n",
    "# instance#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_indices.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(x, y, maxlen, epochs, dimensions):\n",
    "    \"\"\"\n",
    "    Creates and trains a model.\n",
    "    :param x: Numpy boolean array of shape \n",
    "                    (Number of sequences, maxlen, number of distinct character)\n",
    "    :type  x: numpy.ndarray\n",
    "    :param y: Numpy boolean array of shape \n",
    "                    (Number of sequences, number of distinct character)\n",
    "    :type  y: numpy.ndarray\n",
    "    :param maxlen: the length of a sequence to extract as train\n",
    "    :type  maxlen: int\n",
    "    :param epochs: number of training iterations\n",
    "    :type  epochs: int\n",
    "    :param chars: list of unique characters\n",
    "    :type  chars: list\n",
    "    :returns: trained keras model\n",
    "    :rtype:   keras.engine.sequential.Sequential\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.GRU(\n",
    "        32,\n",
    "        return_sequences=True,\n",
    "        input_shape=(maxlen, dimensions))\n",
    "    )\n",
    "    model.add(layers.GRU(\n",
    "        64,\n",
    "        input_shape=(maxlen, dimensions))\n",
    "    )\n",
    "    model.add(layers.Dense(\n",
    "        dimensions, \n",
    "        activation='softmax')\n",
    "    )\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    optimizer = optimizers.RMSprop(lr=0.01)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    model.fit(x, y, batch_size=128, epochs=epochs)\n",
    "\n",
    "    return (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_3 (GRU)                  (None, 10, 32)            12768     \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 64)                18624     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               6500      \n",
      "=================================================================\n",
      "Total params: 37,892\n",
      "Trainable params: 37,892\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "106/106 [==============================] - 1s 12ms/step - loss: nan\n",
      "Epoch 2/3\n",
      "106/106 [==============================] - 0s 166us/step - loss: nan\n",
      "Epoch 3/3\n",
      "106/106 [==============================] - 0s 207us/step - loss: nan\n"
     ]
    }
   ],
   "source": [
    "model = create_model(x, y, maxlen, 3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 53\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_9 (GRU)                  (None, 10, 32)            12768     \n",
      "_________________________________________________________________\n",
      "gru_10 (GRU)                 (None, 64)                18624     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               6500      \n",
      "=================================================================\n",
      "Total params: 37,892\n",
      "Trainable params: 37,892\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "106/106 [==============================] - 1s 13ms/step - loss: nan\n",
      "Epoch 2/3\n",
      "106/106 [==============================] - 0s 168us/step - loss: nan\n",
      "Epoch 3/3\n",
      "106/106 [==============================] - 0s 187us/step - loss: nan\n"
     ]
    }
   ],
   "source": [
    "def train_model_from_lyrics(lyrics, maxlen=10, step=20, epochs=10):\n",
    "    \"\"\"\n",
    "    Given lyrics, train the model.\n",
    "    \n",
    "    :param lyrics: A string with all the lyrics together.\n",
    "    :type  lyrics: str\n",
    "    :param maxlen: the length of a sequence to extract as train\n",
    "    :type  maxlen: int\n",
    "    :param step: sample a new sequence every n steps\n",
    "    :type  step: int\n",
    "    :param epochs: number of training iterations\n",
    "    :type  epochs: int\n",
    "    :returns: (trained keras model,\n",
    "               dictionary mapping characters to digit representations)\n",
    "    :rtype:   (keras.engine.sequential.Sequential,\n",
    "               dict)\n",
    "    \"\"\"\n",
    "    \n",
    "    x, y, word_indices = vectorizing_seq(tokens, maxlen, 3)\n",
    "#     x, y, char_indices = vectorizing_seq(lyrics, maxlen, step)\n",
    "#     chars = list (char_indices.keys())\n",
    "    model = create_model(x, y, maxlen, 3, 100)\n",
    "#     model = create_model(x, y, maxlen, epochs, chars)\n",
    "    \n",
    "    return model, word_indices\n",
    "\n",
    "model, word_indices = train_model_from_lyrics(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Compute new probability distribution based on the temperature\n",
    "    Higher temperature creates more randomness.\n",
    "    \n",
    "    :param preds: numpy array of shape (unique chars,), and elements sum to 1\n",
    "    :type  preds: numpy.ndarray\n",
    "    :param temperature: characterizes the entropy of probability distribution\n",
    "    :type  temperature: float\n",
    "    :returns: a number 0 to the length of preds - 1\n",
    "    :rtype:   int\n",
    "    \"\"\"\n",
    "    \n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_generate(model, text, char_indices, maxlen=60, temperature=1.0, textlen=400):\n",
    "    \"\"\"\n",
    "    Generate text based on a model.\n",
    "    \n",
    "    :param model: trained keras model\n",
    "    :type  model: keras.engine.sequential.Sequential\n",
    "    :param text: lyrics\n",
    "    :type  text: str\n",
    "    :param char_indices: dictionary mapping a character to its integer placeholder\n",
    "    :type  char_indices: dict\n",
    "    :param maxlen: maximum length of the sequences\n",
    "    :type  maxlen: int\n",
    "    :param textlen: Number of characters of generated sequence\n",
    "    :type  textlen: int\n",
    "    \"\"\"\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1) \n",
    "    generated_text = text[start_index: start_index + maxlen] \n",
    "    full_sentence = \" \".join (generated_text)\n",
    "    print('--- Generating with seed: \"' + full_sentence + '\"')\n",
    "    \n",
    "    chars = list (char_indices.keys())\n",
    "    \n",
    "    print('------ temperature:', temperature)\n",
    "    sys.stdout.write(generated_text)\n",
    "    for i in range(textlen):\n",
    "        sampled = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(generated_text):\n",
    "            sampled[0, t, char_indices[char]] = 1\n",
    "        preds = model.predict(sampled, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_char = chars[next_index]\n",
    "        generated_text += next_char\n",
    "        generated_text = generated_text[1:]\n",
    "        sys.stdout.write(next_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
