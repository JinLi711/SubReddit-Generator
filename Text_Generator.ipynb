{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "My Plan:\n",
    "\n",
    "1. Combine all the text together into one long file (one long string).\n",
    "2. Lowercase all the words (one long string)\n",
    "3. Tokenize the words. (list of words split by spaces.)\n",
    "4. Split into two lists, one that holds the sentence (input), \n",
    "   another that holds the predicted next word (label).\n",
    "5. Convert the training sentences into vector representations.\n",
    "6. One hot encode the labels.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, \\\n",
    "    EarlyStopping, ReduceLROnPlateau, TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford's Word2Vec (100 dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_GloVe(directory):\n",
    "    \"\"\"\n",
    "    Open Stanford's GloVe file with 100 dimensional embeddings\n",
    "    \n",
    "    :param directory: directory of the GloVe\n",
    "    :type  directory: str\n",
    "    :return: dictionary where the keys are the words, \n",
    "             and values are the 100d representation\n",
    "    :rtype:  dict\n",
    "    \"\"\"\n",
    "\n",
    "    glove_dir = directory\n",
    "\n",
    "    # dictionary that maps words into 100d array\n",
    "    embeddings_index = {}\n",
    "    file = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    file.close()\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorizing_seq (text, maxlen, step):    \n",
    "    \"\"\"\n",
    "    Convert the text into inputs and labels.\n",
    "    \n",
    "    :param text: list of words\n",
    "    :type  text: list\n",
    "    :param maxlen: the length of a sequence to extract as train\n",
    "    :type  maxlen: int\n",
    "    :param step: sample a new sequence every n steps\n",
    "    :type  step: int\n",
    "    :returns: (Numpy boolean array of shape \n",
    "                    (Number of sequences, maxlen, number of distinct character),\n",
    "               Numpy boolean array of shape \n",
    "                    (Number of sequences, number of distinct character),\n",
    "               dictionary mapping a integer to its character placeholder)\n",
    "    :rtype:   (numpy.ndarray, \n",
    "               numpy.ndarray, \n",
    "               dict)     \n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = [] # hold extracted sequences\n",
    "    next_word = [] # hold next word for each corresponding sentence\n",
    "\n",
    "    for i in range(0, len(text) - maxlen, step):\n",
    "        sentences.append(text[i: i + maxlen])\n",
    "        next_word.append(text[i + maxlen])\n",
    "\n",
    "    print('Number of sequences:', len(sentences))\n",
    "\n",
    "    all_words = sorted(list(set(text)))\n",
    "    \n",
    "    word_indices = {}\n",
    "    for word in all_words:\n",
    "        try:\n",
    "            word_indices[word] = word_vectors[word]\n",
    "        except KeyError:\n",
    "            word_indices[word] = np.zeros(100)\n",
    "            \n",
    "    x = np.empty((0, maxlen, 100), float)\n",
    "    y = np.array (next_word)\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        instance = []\n",
    "        for t, word in enumerate(sentence):\n",
    "            word_dimensions = list (word_indices[word])\n",
    "            instance.append(word_dimensions)\n",
    "        instance = np.array(instance)\n",
    "        instance = np.reshape(instance, (1,) + instance.shape ) \n",
    "        x = np.append(x, instance, axis=0)\n",
    "\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    enc = OneHotEncoder(sparse=False)\n",
    "    y = enc.fit_transform(y.reshape(-1,1))\n",
    "    \n",
    "    needed_words = enc.categories_[0]\n",
    "    word_indices2 = dict(( i, word) for i, word in enumerate (needed_words))\n",
    "    return x, y, word_indices, word_indices2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_path = \"{}_weights.best.hdf5\".format('RNN')\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    weight_path,\n",
    "    monitor='loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "early = EarlyStopping(\n",
    "    monitor=\"loss\",\n",
    "    #monitor='acc'\n",
    "    mode=\"min\",\n",
    "    verbose=2,\n",
    "    # training is interrupted when the monitor argument stops improving after n steps\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "callbacks_list = [checkpoint, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(x, y, maxlen, epochs, dimensions):\n",
    "    \"\"\"\n",
    "    Creates and trains a model.\n",
    "    :param x: Numpy boolean array of shape \n",
    "                    (Number of sequences, maxlen, number of distinct character)\n",
    "    :type  x: numpy.ndarray\n",
    "    :param y: Numpy boolean array of shape \n",
    "                    (Number of sequences, number of distinct character)\n",
    "    :type  y: numpy.ndarray\n",
    "    :param maxlen: the length of a sequence to extract as train\n",
    "    :type  maxlen: int\n",
    "    :param epochs: number of training iterations\n",
    "    :type  epochs: int\n",
    "    :param chars: list of unique characters\n",
    "    :type  chars: list\n",
    "    :returns: trained keras model\n",
    "    :rtype:   keras.engine.sequential.Sequential\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.GRU(\n",
    "        32,\n",
    "        return_sequences=True,\n",
    "        input_shape=(maxlen, 100))\n",
    "    )\n",
    "    model.add(layers.GRU(\n",
    "        64,\n",
    "        input_shape=(maxlen, dimensions))\n",
    "    )\n",
    "    model.add(layers.Dense(\n",
    "        dimensions, \n",
    "        activation='softmax')\n",
    "    )\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    optimizer = optimizers.RMSprop(lr=0.01)\n",
    "#     optimizer = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "#     model.compile(loss='mse', optimizer=optimizer)\n",
    "    model.fit(x, y, batch_size=128, epochs=epochs, callbacks=callbacks_list)\n",
    "\n",
    "    return (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model_from_text(text, maxlen=10, step=20, epochs=10):\n",
    "    \"\"\"\n",
    "    Given text, train the model.\n",
    "    \n",
    "    :param text: A string with all the text together.\n",
    "    :type  text: str\n",
    "    :param maxlen: the length of a sequence to extract as train\n",
    "    :type  maxlen: int\n",
    "    :param step: sample a new sequence every n steps\n",
    "    :type  step: int\n",
    "    :param epochs: number of training iterations\n",
    "    :type  epochs: int\n",
    "    :returns: (trained keras model,\n",
    "               dictionary mapping characters to digit representations)\n",
    "    :rtype:   (keras.engine.sequential.Sequential,\n",
    "               dict)\n",
    "    \"\"\"\n",
    "    \n",
    "    x, y, wordvectors_mini, word_indices = vectorizing_seq(tokens, maxlen, 3)\n",
    "    model = create_model(x, y, maxlen, 3, y.shape[1])\n",
    "    \n",
    "    return model, word_indices, wordvectors_mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redistribute Probability Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Compute new probability distribution based on the temperature\n",
    "    Higher temperature creates more randomness.\n",
    "    \n",
    "    :param preds: numpy array of shape (unique chars,), and elements sum to 1\n",
    "    :type  preds: numpy.ndarray\n",
    "    :param temperature: characterizes the entropy of probability distribution\n",
    "    :type  temperature: float\n",
    "    :returns: a number 0 to the length of preds - 1\n",
    "    :rtype:   int\n",
    "    \"\"\"\n",
    "    \n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_generate(model, text, word_indices, maxlen=10, temperature=1.0, textlen=40):\n",
    "    \"\"\"\n",
    "    Generate text based on a model.\n",
    "    \n",
    "    :param model: trained keras model\n",
    "    :type  model: keras.engine.sequential.Sequential\n",
    "    :param text: lyrics\n",
    "    :type  text: str\n",
    "    :param char_indices: dictionary mapping a character to its integer placeholder\n",
    "    :type  char_indices: dict\n",
    "    :param maxlen: maximum length of the sequences\n",
    "    :type  maxlen: int\n",
    "    :param textlen: Number of characters of generated sequence\n",
    "    :type  textlen: int\n",
    "    \"\"\"\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1) \n",
    "    generated_text = text[start_index: start_index + maxlen] \n",
    "    full_sentence = \" \".join (generated_text)\n",
    "    print(len(generated_text))\n",
    "    print('--- Generating with seed: \"' + full_sentence + '\"')\n",
    "    \n",
    "    print('------ temperature:', temperature)\n",
    "    sys.stdout.write(full_sentence)\n",
    "    \n",
    "    \n",
    "    for i in range(textlen):\n",
    "        \n",
    "        sampled = []\n",
    "        for t, word in enumerate(generated_text):\n",
    "            word_dimensions = list (wordvectors_mini[word])\n",
    "            sampled.append(word_dimensions)\n",
    "        sampled = np.array(sampled)\n",
    "        sampled = np.reshape(sampled, (1,) + sampled.shape ) \n",
    "\n",
    "        preds = model.predict(sampled, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_word = word_indices[next_index]\n",
    "\n",
    "        generated_text.append ( next_word)\n",
    "        generated_text = generated_text[1:]\n",
    "        sys.stdout.write(\" \" + next_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "directory = '/Users/jinli/Projects/glove.6B'\n",
    "word_vectors = get_GloVe(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = open('all.txt', 'r').read()\n",
    "text = text.lower()\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "# small amount for now\n",
    "tokens = token[:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token\n",
    "# maxlen = 10\n",
    "# x, y, word_indices = vectorizing_seq(tokens, maxlen, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 1330\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_13 (GRU)                 (None, 10, 32)            12768     \n",
      "_________________________________________________________________\n",
      "gru_14 (GRU)                 (None, 64)                18624     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 598)               38870     \n",
      "=================================================================\n",
      "Total params: 70,262\n",
      "Trainable params: 70,262\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "1330/1330 [==============================] - 2s 2ms/step - loss: 6.2140\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.21399, saving model to CNN2_weights.best.hdf5\n",
      "Epoch 2/3\n",
      "1330/1330 [==============================] - 0s 202us/step - loss: 5.8209\n",
      "\n",
      "Epoch 00002: loss improved from 6.21399 to 5.82090, saving model to CNN2_weights.best.hdf5\n",
      "Epoch 3/3\n",
      "1330/1330 [==============================] - 0s 234us/step - loss: 5.6827\n",
      "\n",
      "Epoch 00003: loss improved from 5.82090 to 5.68270, saving model to CNN2_weights.best.hdf5\n"
     ]
    }
   ],
   "source": [
    "# model = create_model(x, y, maxlen, epochs=3, dimensions=y.shape[1])\n",
    "model, word_indices, wordvectors_mini = train_model_from_text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "--- Generating with seed: \"ve had a coffee , the key is not to\"\n",
      "------ temperature: 1.0\n",
      "ve had a coffee , the key is not to be walking right i think think when . via by the ever is high ill snickers story in has limo master inter-connecting gameplay hand ive male next previous feelings profit seen down be by internet told along inter-connecting suffered of"
     ]
    }
   ],
   "source": [
    "text_generate(model, tokens, word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to load the model\n",
    "#model = load_model('my_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
