{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "My Plan:\n",
    "\n",
    "1. Combine all the text together into one long file (one long string).\n",
    "2. Lowercase all the words (one long string)\n",
    "3. Tokenize the words. (list of words split by spaces.)\n",
    "4. Split into two lists, one that holds the sentence (input), \n",
    "   another that holds the predicted next word (label).\n",
    "5. Convert the training sentences into vector representations.\n",
    "6. One hot encode the labels.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, \\\n",
    "    EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "# import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford's Word2Vec (100 dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_GloVe(directory):\n",
    "    \"\"\"\n",
    "    Open Stanford's GloVe file with 100 dimensional embeddings\n",
    "    \n",
    "    :param directory: directory of the GloVe\n",
    "    :type  directory: str\n",
    "    :return: dictionary where the keys are the words, \n",
    "             and values are the 100d representation\n",
    "    :rtype:  dict\n",
    "    \"\"\"\n",
    "\n",
    "    glove_dir = directory\n",
    "\n",
    "    # dictionary that maps words into 100d array\n",
    "    embeddings_index = {}\n",
    "    file = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    file.close()\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorizing_seq (text, maxlen, step):    \n",
    "    \"\"\"\n",
    "    Convert the text into inputs and labels.\n",
    "    \n",
    "    :param text: list of words\n",
    "    :type  text: list\n",
    "    :param maxlen: the length of a sequence to extract as train\n",
    "    :type  maxlen: int\n",
    "    :param step: sample a new sequence every n steps\n",
    "    :type  step: int\n",
    "    :returns: (Numpy boolean array of shape \n",
    "                    (Number of sequences, maxlen, number of distinct character),\n",
    "               Numpy boolean array of shape \n",
    "                    (Number of sequences, number of distinct character),\n",
    "               dictionary mapping a integer to its character placeholder)\n",
    "    :rtype:   (numpy.ndarray, \n",
    "               numpy.ndarray, \n",
    "               dict)     \n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = [] # hold extracted sequences\n",
    "    next_word = [] # hold next word for each corresponding sentence\n",
    "\n",
    "    for i in range(0, len(text) - maxlen, step):\n",
    "        sentences.append(text[i: i + maxlen])\n",
    "        next_word.append(text[i + maxlen])\n",
    "\n",
    "    print('Number of sequences:', len(sentences))\n",
    "\n",
    "    all_words = sorted(list(set(text)))\n",
    "    \n",
    "    word_indices = {}\n",
    "    for word in all_words:\n",
    "        try:\n",
    "            word_indices[word] = word_vectors[word]\n",
    "        except KeyError:\n",
    "            word_indices[word] = np.zeros(100)\n",
    "            \n",
    "    x = np.empty((0, maxlen, 100), float)\n",
    "    y = np.array (next_word)\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        instance = []\n",
    "        for t, word in enumerate(sentence):\n",
    "            word_dimensions = list (word_indices[word])\n",
    "            instance.append(word_dimensions)\n",
    "        instance = np.array(instance)\n",
    "        instance = np.reshape(instance, (1,) + instance.shape ) \n",
    "        x = np.append(x, instance, axis=0)\n",
    "\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    enc = OneHotEncoder(sparse=False)\n",
    "    y = enc.fit_transform(y.reshape(-1,1))\n",
    "    \n",
    "    needed_words = enc.categories_[0]\n",
    "    word_indices2 = dict(( i, word) for i, word in enumerate (needed_words))\n",
    "    return x, y, word_indices, word_indices2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_path = \"{}_weights.best.hdf5\".format('RNN')\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    weight_path,\n",
    "    monitor='loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "early = EarlyStopping(\n",
    "    monitor=\"loss\",\n",
    "    #monitor='acc'\n",
    "    mode=\"min\",\n",
    "    verbose=2,\n",
    "    # training is interrupted when the monitor argument stops improving after n steps\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "callbacks_list = [checkpoint, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(x, y, maxlen, epochs, dimensions):\n",
    "    \"\"\"\n",
    "    Creates and trains a model.\n",
    "    :param x: Numpy boolean array of shape \n",
    "                    (Number of sequences, maxlen, number of distinct character)\n",
    "    :type  x: numpy.ndarray\n",
    "    :param y: Numpy boolean array of shape \n",
    "                    (Number of sequences, number of distinct character)\n",
    "    :type  y: numpy.ndarray\n",
    "    :param maxlen: the length of a sequence to extract as train\n",
    "    :type  maxlen: int\n",
    "    :param epochs: number of training iterations\n",
    "    :type  epochs: int\n",
    "    :param chars: list of unique characters\n",
    "    :type  chars: list\n",
    "    :returns: trained keras model\n",
    "    :rtype:   keras.engine.sequential.Sequential\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.GRU(\n",
    "        32,\n",
    "        return_sequences=True,\n",
    "        input_shape=(maxlen, 100))\n",
    "    )\n",
    "    model.add(layers.GRU(\n",
    "        64,\n",
    "        input_shape=(maxlen, dimensions))\n",
    "    )\n",
    "    model.add(layers.Dense(\n",
    "        dimensions, \n",
    "        activation='softmax')\n",
    "    )\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    optimizer = optimizers.RMSprop(lr=0.01)\n",
    "#     optimizer = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "#     model.compile(loss='mse', optimizer=optimizer)\n",
    "    model.fit(x, y, batch_size=128, epochs=epochs, callbacks=callbacks_list)\n",
    "\n",
    "    return (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model_from_text(text, maxlen=10, step=20, epochs=10):\n",
    "    \"\"\"\n",
    "    Given text, train the model.\n",
    "    \n",
    "    :param text: A string with all the text together.\n",
    "    :type  text: str\n",
    "    :param maxlen: the length of a sequence to extract as train\n",
    "    :type  maxlen: int\n",
    "    :param step: sample a new sequence every n steps\n",
    "    :type  step: int\n",
    "    :param epochs: number of training iterations\n",
    "    :type  epochs: int\n",
    "    :returns: (trained keras model,\n",
    "               dictionary mapping characters to digit representations)\n",
    "    :rtype:   (keras.engine.sequential.Sequential,\n",
    "               dict)\n",
    "    \"\"\"\n",
    "    \n",
    "    x, y, wordvectors_mini, word_indices = vectorizing_seq(tokens, maxlen, 3)\n",
    "    model = create_model(x, y, maxlen, 3, y.shape[1])\n",
    "    \n",
    "    return model, word_indices, wordvectors_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36968"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof (wordvectors_mini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redistribute Probability Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Compute new probability distribution based on the temperature\n",
    "    Higher temperature creates more randomness.\n",
    "    \n",
    "    :param preds: numpy array of shape (unique chars,), and elements sum to 1\n",
    "    :type  preds: numpy.ndarray\n",
    "    :param temperature: characterizes the entropy of probability distribution\n",
    "    :type  temperature: float\n",
    "    :returns: a number 0 to the length of preds - 1\n",
    "    :rtype:   int\n",
    "    \"\"\"\n",
    "    \n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_generate(model, text, word_indices, maxlen=10, temperature=1.0, textlen=40):\n",
    "    \"\"\"\n",
    "    Generate text based on a model.\n",
    "    \n",
    "    :param model: trained keras model\n",
    "    :type  model: keras.engine.sequential.Sequential\n",
    "    :param text: lyrics\n",
    "    :type  text: str\n",
    "    :param char_indices: dictionary mapping a character to its integer placeholder\n",
    "    :type  char_indices: dict\n",
    "    :param maxlen: maximum length of the sequences\n",
    "    :type  maxlen: int\n",
    "    :param textlen: Number of characters of generated sequence\n",
    "    :type  textlen: int\n",
    "    \"\"\"\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1) \n",
    "    generated_text = text[start_index: start_index + maxlen] \n",
    "    full_sentence = \" \".join (generated_text)\n",
    "    print(len(generated_text))\n",
    "    print('--- Generating with seed: \"' + full_sentence + '\"')\n",
    "    \n",
    "    print('------ temperature:', temperature)\n",
    "    sys.stdout.write(full_sentence)\n",
    "    \n",
    "    \n",
    "    for i in range(textlen):\n",
    "        \n",
    "        sampled = []\n",
    "        for t, word in enumerate(generated_text):\n",
    "            word_dimensions = list (wordvectors_mini[word])\n",
    "            sampled.append(word_dimensions)\n",
    "        sampled = np.array(sampled)\n",
    "        sampled = np.reshape(sampled, (1,) + sampled.shape ) \n",
    "\n",
    "        preds = model.predict(sampled, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_word = word_indices[next_index]\n",
    "\n",
    "        generated_text.append ( next_word)\n",
    "        generated_text = generated_text[1:]\n",
    "        sys.stdout.write(\" \" + next_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "directory = '/Users/jinli/Projects/glove.6B'\n",
    "word_vectors = get_GloVe(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = open('all.txt', 'r').read()\n",
    "text = text.lower()\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "# small amount for now\n",
    "tokens = token[:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '!',\n",
       " 1: '#',\n",
       " 2: '%',\n",
       " 3: '(',\n",
       " 4: ')',\n",
       " 5: '*rotational',\n",
       " 6: '*still*',\n",
       " 7: '*unsprung',\n",
       " 8: '+',\n",
       " 9: ',',\n",
       " 10: '-',\n",
       " 11: '.',\n",
       " 12: '...',\n",
       " 13: '//i.imgur.com/qnniqul.jpg',\n",
       " 14: '1',\n",
       " 15: '1,000',\n",
       " 16: '10',\n",
       " 17: '1018',\n",
       " 18: '2.',\n",
       " 19: '20',\n",
       " 20: '40',\n",
       " 21: '40.',\n",
       " 22: '5',\n",
       " 23: '6',\n",
       " 24: '8',\n",
       " 25: '95',\n",
       " 26: ':',\n",
       " 27: '=',\n",
       " 28: '?',\n",
       " 29: 'a',\n",
       " 30: 'able',\n",
       " 31: 'about',\n",
       " 32: 'absorbed',\n",
       " 33: 'accept',\n",
       " 34: 'accidents',\n",
       " 35: 'ache',\n",
       " 36: 'actual',\n",
       " 37: 'actually',\n",
       " 38: 'ad',\n",
       " 39: 'adderall',\n",
       " 40: 'afraid',\n",
       " 41: 'africa',\n",
       " 42: 'after',\n",
       " 43: 'again',\n",
       " 44: 'agree',\n",
       " 45: 'all',\n",
       " 46: 'allow',\n",
       " 47: 'almost',\n",
       " 48: 'along',\n",
       " 49: 'already',\n",
       " 50: 'always',\n",
       " 51: 'am',\n",
       " 52: 'an',\n",
       " 53: 'and',\n",
       " 54: 'annoyed',\n",
       " 55: 'any',\n",
       " 56: 'applause',\n",
       " 57: 'are',\n",
       " 58: 'arse',\n",
       " 59: 'as',\n",
       " 60: 'ash',\n",
       " 61: 'at',\n",
       " 62: 'ate',\n",
       " 63: 'attention',\n",
       " 64: 'avoidance',\n",
       " 65: 'away',\n",
       " 66: 'back',\n",
       " 67: 'bad',\n",
       " 68: 'based',\n",
       " 69: 'be',\n",
       " 70: 'because',\n",
       " 71: 'bed',\n",
       " 72: 'been',\n",
       " 73: 'before',\n",
       " 74: 'began',\n",
       " 75: 'being',\n",
       " 76: 'below',\n",
       " 77: 'best',\n",
       " 78: 'bet',\n",
       " 79: 'better',\n",
       " 80: 'between',\n",
       " 81: 'big',\n",
       " 82: 'bodies',\n",
       " 83: 'bombarded',\n",
       " 84: 'book',\n",
       " 85: 'born',\n",
       " 86: 'braille',\n",
       " 87: 'bruce',\n",
       " 88: 'bullshit',\n",
       " 89: 'but',\n",
       " 90: 'button',\n",
       " 91: 'by',\n",
       " 92: 'c',\n",
       " 93: 'can',\n",
       " 94: 'cancer',\n",
       " 95: 'candy',\n",
       " 96: 'cant',\n",
       " 97: 'car',\n",
       " 98: 'cardboard',\n",
       " 99: 'care',\n",
       " 100: 'career',\n",
       " 101: 'carrey',\n",
       " 102: 'cars',\n",
       " 103: 'catalogs',\n",
       " 104: 'chain',\n",
       " 105: 'character',\n",
       " 106: 'checks',\n",
       " 107: 'cheese',\n",
       " 108: 'chicken',\n",
       " 109: 'chubby',\n",
       " 110: 'circle',\n",
       " 111: 'clean',\n",
       " 112: 'clear',\n",
       " 113: 'close',\n",
       " 114: 'clothes',\n",
       " 115: 'communicated',\n",
       " 116: 'considered',\n",
       " 117: 'cool',\n",
       " 118: 'core',\n",
       " 119: 'could',\n",
       " 120: 'counterproductive',\n",
       " 121: 'couple',\n",
       " 122: 'course',\n",
       " 123: 'created',\n",
       " 124: 'credit',\n",
       " 125: 'crowd',\n",
       " 126: 'ctrl',\n",
       " 127: 'dad',\n",
       " 128: 'danny',\n",
       " 129: 'daughters',\n",
       " 130: 'dave',\n",
       " 131: 'day',\n",
       " 132: 'deadline',\n",
       " 133: 'dealerships',\n",
       " 134: 'deleted',\n",
       " 135: 'deliberately',\n",
       " 136: 'dense',\n",
       " 137: 'describes',\n",
       " 138: 'destroying',\n",
       " 139: 'devs',\n",
       " 140: 'dialogue',\n",
       " 141: 'did',\n",
       " 142: 'didnt',\n",
       " 143: 'do',\n",
       " 144: 'doctor',\n",
       " 145: 'does',\n",
       " 146: 'doesnt',\n",
       " 147: 'dog',\n",
       " 148: 'dogs',\n",
       " 149: 'doing',\n",
       " 150: 'dont',\n",
       " 151: 'door',\n",
       " 152: 'down',\n",
       " 153: 'dreaming',\n",
       " 154: 'dress',\n",
       " 155: 'drink',\n",
       " 156: 'drove',\n",
       " 157: 'during',\n",
       " 158: 'eachother',\n",
       " 159: 'earth',\n",
       " 160: 'easier',\n",
       " 161: 'eat',\n",
       " 162: 'eccentric',\n",
       " 163: 'eddie',\n",
       " 164: 'effort',\n",
       " 165: 'emphysema',\n",
       " 166: 'end',\n",
       " 167: 'energy',\n",
       " 168: 'english',\n",
       " 169: 'enough',\n",
       " 170: 'entire',\n",
       " 171: 'epic',\n",
       " 172: 'eternal',\n",
       " 173: 'even',\n",
       " 174: 'events',\n",
       " 175: 'ever',\n",
       " 176: 'every',\n",
       " 177: 'everyone',\n",
       " 178: 'evian',\n",
       " 179: 'ex',\n",
       " 180: 'example',\n",
       " 181: 'excuse',\n",
       " 182: 'existed',\n",
       " 183: 'expect',\n",
       " 184: 'exploit',\n",
       " 185: 'extras',\n",
       " 186: 'factoid',\n",
       " 187: 'family',\n",
       " 188: 'famous',\n",
       " 189: 'fancy',\n",
       " 190: 'fans',\n",
       " 191: 'fantasy',\n",
       " 192: 'fat',\n",
       " 193: 'feelings',\n",
       " 194: 'first',\n",
       " 195: 'fitness',\n",
       " 196: 'flight',\n",
       " 197: 'flower',\n",
       " 198: 'fluency',\n",
       " 199: 'follow',\n",
       " 200: 'follow-up',\n",
       " 201: 'food',\n",
       " 202: 'for',\n",
       " 203: 'frank',\n",
       " 204: 'freezing',\n",
       " 205: 'friendlys',\n",
       " 206: 'from',\n",
       " 207: 'functioning',\n",
       " 208: 'fundraisers',\n",
       " 209: 'funeral',\n",
       " 210: 'future',\n",
       " 211: 'gameplay',\n",
       " 212: 'gave',\n",
       " 213: 'generally',\n",
       " 214: 'get',\n",
       " 215: 'getting',\n",
       " 216: 'giving',\n",
       " 217: 'go',\n",
       " 218: 'goes',\n",
       " 219: 'going',\n",
       " 220: 'gon',\n",
       " 221: 'google',\n",
       " 222: 'great',\n",
       " 223: 'grounded',\n",
       " 224: 'group',\n",
       " 225: 'grows',\n",
       " 226: 'guardians',\n",
       " 227: 'guide',\n",
       " 228: 'guy',\n",
       " 229: 'had',\n",
       " 230: 'hammer',\n",
       " 231: 'hand',\n",
       " 232: 'has',\n",
       " 233: 'have',\n",
       " 234: 'havent',\n",
       " 235: 'he',\n",
       " 236: 'head',\n",
       " 237: 'hear',\n",
       " 238: 'heard',\n",
       " 239: 'hell',\n",
       " 240: 'help',\n",
       " 241: 'helpful',\n",
       " 242: 'her',\n",
       " 243: 'hetero',\n",
       " 244: 'high',\n",
       " 245: 'higher',\n",
       " 246: 'higher-dimensional',\n",
       " 247: 'his',\n",
       " 248: 'holiday',\n",
       " 249: 'home',\n",
       " 250: 'how',\n",
       " 251: 'huge',\n",
       " 252: 'hurtling',\n",
       " 253: 'i',\n",
       " 254: 'idea',\n",
       " 255: 'if',\n",
       " 256: 'ill',\n",
       " 257: 'im',\n",
       " 258: 'image',\n",
       " 259: 'images',\n",
       " 260: 'imagine',\n",
       " 261: 'impressive',\n",
       " 262: 'in',\n",
       " 263: 'infestation',\n",
       " 264: 'insipidus',\n",
       " 265: 'inter-connecting',\n",
       " 266: 'internet',\n",
       " 267: 'interstellar',\n",
       " 268: 'interview',\n",
       " 269: 'into',\n",
       " 270: 'invention',\n",
       " 271: 'is',\n",
       " 272: 'isolated',\n",
       " 273: 'it',\n",
       " 274: 'itll',\n",
       " 275: 'its',\n",
       " 276: 'ive',\n",
       " 277: 'janitors',\n",
       " 278: 'jedi',\n",
       " 279: 'jon',\n",
       " 280: 'june',\n",
       " 281: 'just',\n",
       " 282: 'kardashian',\n",
       " 283: 'kart',\n",
       " 284: 'kept',\n",
       " 285: 'key',\n",
       " 286: 'kinda',\n",
       " 287: 'kissing',\n",
       " 288: 'knew',\n",
       " 289: 'know',\n",
       " 290: 'knowing',\n",
       " 291: 'knowledge',\n",
       " 292: 'language',\n",
       " 293: 'lapras',\n",
       " 294: 'last',\n",
       " 295: 'least',\n",
       " 296: 'leave',\n",
       " 297: 'lego',\n",
       " 298: 'letting',\n",
       " 299: 'life',\n",
       " 300: 'light',\n",
       " 301: 'lightsaber',\n",
       " 302: 'like',\n",
       " 303: 'limited',\n",
       " 304: 'limo',\n",
       " 305: 'literal',\n",
       " 306: 'literally',\n",
       " 307: 'little',\n",
       " 308: 'locking',\n",
       " 309: 'long',\n",
       " 310: 'longer',\n",
       " 311: 'look',\n",
       " 312: 'looking',\n",
       " 313: 'lot',\n",
       " 314: 'louis',\n",
       " 315: 'm',\n",
       " 316: 'make',\n",
       " 317: 'makes',\n",
       " 318: 'male',\n",
       " 319: 'man',\n",
       " 320: 'mario',\n",
       " 321: 'markers',\n",
       " 322: 'master',\n",
       " 323: 'mates',\n",
       " 324: 'maximum',\n",
       " 325: 'may',\n",
       " 326: 'maybe',\n",
       " 327: 'mcdonalds',\n",
       " 328: 'me',\n",
       " 329: 'mean',\n",
       " 330: 'meaning',\n",
       " 331: 'measure',\n",
       " 332: 'member',\n",
       " 333: 'meme',\n",
       " 334: 'memory',\n",
       " 335: 'message',\n",
       " 336: 'might',\n",
       " 337: 'millions',\n",
       " 338: 'minutes',\n",
       " 339: 'mistake',\n",
       " 340: 'model',\n",
       " 341: 'monologue',\n",
       " 342: 'more',\n",
       " 343: 'mosquitoes',\n",
       " 344: 'most',\n",
       " 345: 'movements',\n",
       " 346: 'movie',\n",
       " 347: 'must',\n",
       " 348: 'my',\n",
       " 349: 'na',\n",
       " 350: 'nature',\n",
       " 351: 'never',\n",
       " 352: 'newly-created',\n",
       " 353: 'next',\n",
       " 354: 'nice',\n",
       " 355: 'night',\n",
       " 356: 'no',\n",
       " 357: 'not',\n",
       " 358: 'nothing',\n",
       " 359: 'nuts',\n",
       " 360: 'occurs',\n",
       " 361: 'of',\n",
       " 362: 'off',\n",
       " 363: 'okay',\n",
       " 364: 'old',\n",
       " 365: 'older',\n",
       " 366: 'on',\n",
       " 367: 'once',\n",
       " 368: 'one',\n",
       " 369: 'online',\n",
       " 370: 'only',\n",
       " 371: 'opposite',\n",
       " 372: 'or',\n",
       " 373: 'other',\n",
       " 374: 'out',\n",
       " 375: 'outright',\n",
       " 376: 'outside',\n",
       " 377: 'outward',\n",
       " 378: 'pads',\n",
       " 379: 'part',\n",
       " 380: 'particle',\n",
       " 381: 'parties',\n",
       " 382: 'people',\n",
       " 383: 'person',\n",
       " 384: 'phone',\n",
       " 385: 'photon',\n",
       " 386: 'pinterest',\n",
       " 387: 'pizza',\n",
       " 388: 'place',\n",
       " 389: 'placement',\n",
       " 390: 'plug',\n",
       " 391: 'pneumonia',\n",
       " 392: 'pointed',\n",
       " 393: 'pokemon',\n",
       " 394: 'police',\n",
       " 395: 'pooping',\n",
       " 396: 'pop',\n",
       " 397: 'population',\n",
       " 398: 'possible',\n",
       " 399: 'potential',\n",
       " 400: 'potion',\n",
       " 401: 'power',\n",
       " 402: 'precious',\n",
       " 403: 'present',\n",
       " 404: 'press',\n",
       " 405: 'pretending',\n",
       " 406: 'pretty',\n",
       " 407: 'previous',\n",
       " 408: 'printing',\n",
       " 409: 'prize',\n",
       " 410: 'probably',\n",
       " 411: 'production',\n",
       " 412: 'profit',\n",
       " 413: 'put',\n",
       " 414: 'pyramids',\n",
       " 415: 'queen',\n",
       " 416: 're',\n",
       " 417: 'reach',\n",
       " 418: 'read',\n",
       " 419: 'real',\n",
       " 420: 'really',\n",
       " 421: 'reap',\n",
       " 422: 'reasoning',\n",
       " 423: 'recently',\n",
       " 424: 'refer',\n",
       " 425: 'regards',\n",
       " 426: 'rehab',\n",
       " 427: 'released',\n",
       " 428: 'remembers',\n",
       " 429: 'respirator',\n",
       " 430: 'rest',\n",
       " 431: 'results',\n",
       " 432: 'reverse',\n",
       " 433: 'review',\n",
       " 434: 'rice',\n",
       " 435: 'riding',\n",
       " 436: 'right',\n",
       " 437: 'ruining',\n",
       " 438: 's',\n",
       " 439: 'said',\n",
       " 440: 'same',\n",
       " 441: 'sand',\n",
       " 442: 'saved',\n",
       " 443: 'say',\n",
       " 444: 'saying',\n",
       " 445: 'school',\n",
       " 446: 'see',\n",
       " 447: 'seen',\n",
       " 448: 'separate',\n",
       " 449: 'sex',\n",
       " 450: 'shamed',\n",
       " 451: 'shamings',\n",
       " 452: 'shes',\n",
       " 453: 'shiny',\n",
       " 454: 'shitty',\n",
       " 455: 'show',\n",
       " 456: 'shower',\n",
       " 457: 'since',\n",
       " 458: 'slapping',\n",
       " 459: 'sleep',\n",
       " 460: 'sleeper',\n",
       " 461: 'smelly',\n",
       " 462: 'snape',\n",
       " 463: 'snickers',\n",
       " 464: 'snowden',\n",
       " 465: 'so',\n",
       " 466: 'some',\n",
       " 467: 'someone',\n",
       " 468: 'something',\n",
       " 469: 'sometimes',\n",
       " 470: 'sorry',\n",
       " 471: 'space',\n",
       " 472: 'speak',\n",
       " 473: 'speaks',\n",
       " 474: 'specifically',\n",
       " 475: 'speed',\n",
       " 476: 'spelled',\n",
       " 477: 'stall',\n",
       " 478: 'staring',\n",
       " 479: 'start',\n",
       " 480: 'steals',\n",
       " 481: 'stick',\n",
       " 482: 'stock',\n",
       " 483: 'stomp',\n",
       " 484: 'stone',\n",
       " 485: 'stop',\n",
       " 486: 'stopped',\n",
       " 487: 'story',\n",
       " 488: 'strict',\n",
       " 489: 'strip',\n",
       " 490: 'strong',\n",
       " 491: 'stuff',\n",
       " 492: 'suffered',\n",
       " 493: 'sun',\n",
       " 494: 'support',\n",
       " 495: 'surface',\n",
       " 496: 'survival',\n",
       " 497: 'susceptible',\n",
       " 498: 'synonyms',\n",
       " 499: 't',\n",
       " 500: 'takes',\n",
       " 501: 'talk',\n",
       " 502: 'talking',\n",
       " 503: 'teacher',\n",
       " 504: 'technically',\n",
       " 505: 'than',\n",
       " 506: 'thanks',\n",
       " 507: 'that',\n",
       " 508: 'thats',\n",
       " 509: 'the',\n",
       " 510: 'their',\n",
       " 511: 'them',\n",
       " 512: 'them..',\n",
       " 513: 'themselves',\n",
       " 514: 'then',\n",
       " 515: 'there',\n",
       " 516: 'theres',\n",
       " 517: 'thermal',\n",
       " 518: 'these',\n",
       " 519: 'they',\n",
       " 520: 'thing',\n",
       " 521: 'think',\n",
       " 522: 'thinks',\n",
       " 523: 'this',\n",
       " 524: 'those',\n",
       " 525: 'thought',\n",
       " 526: 'thousands',\n",
       " 527: 'through',\n",
       " 528: 'throws',\n",
       " 529: 'thus',\n",
       " 530: 'time',\n",
       " 531: 'to',\n",
       " 532: 'today',\n",
       " 533: 'together',\n",
       " 534: 'told',\n",
       " 535: 'treated',\n",
       " 536: 'tree',\n",
       " 537: 'truth',\n",
       " 538: 'try',\n",
       " 539: 'u',\n",
       " 540: 'ultimate',\n",
       " 541: 'underrated',\n",
       " 542: 'universe',\n",
       " 543: 'up',\n",
       " 544: 'us',\n",
       " 545: 'use',\n",
       " 546: 'using',\n",
       " 547: 'usually',\n",
       " 548: 'v',\n",
       " 549: 'value',\n",
       " 550: 've',\n",
       " 551: 'very',\n",
       " 552: 'via',\n",
       " 553: 'vice',\n",
       " 554: 'wait',\n",
       " 555: 'walked',\n",
       " 556: 'walking',\n",
       " 557: 'want',\n",
       " 558: 'wanted',\n",
       " 559: 'was',\n",
       " 560: 'wasnt',\n",
       " 561: 'way',\n",
       " 562: 'wd40',\n",
       " 563: 'wears',\n",
       " 564: 'wed',\n",
       " 565: 'weird',\n",
       " 566: 'well',\n",
       " 567: 'went',\n",
       " 568: 'were',\n",
       " 569: 'what',\n",
       " 570: 'when',\n",
       " 571: 'whereas',\n",
       " 572: 'which',\n",
       " 573: 'while',\n",
       " 574: 'white',\n",
       " 575: 'who',\n",
       " 576: 'wife',\n",
       " 577: 'wild',\n",
       " 578: 'will',\n",
       " 579: 'windmill',\n",
       " 580: 'wings',\n",
       " 581: 'with',\n",
       " 582: 'without',\n",
       " 583: 'wonders',\n",
       " 584: 'worked',\n",
       " 585: 'world',\n",
       " 586: 'worn',\n",
       " 587: 'would',\n",
       " 588: 'wouldn',\n",
       " 589: 'years',\n",
       " 590: 'yes',\n",
       " 591: 'you',\n",
       " 592: 'your',\n",
       " 593: 'youre',\n",
       " 594: 'zeldas',\n",
       " 595: '’',\n",
       " 596: '“',\n",
       " 597: '”'}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# token\n",
    "# maxlen = 10\n",
    "# x, y, word_indices = vectorizing_seq(tokens, maxlen, 3)\n",
    "word_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 1330\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_13 (GRU)                 (None, 10, 32)            12768     \n",
      "_________________________________________________________________\n",
      "gru_14 (GRU)                 (None, 64)                18624     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 598)               38870     \n",
      "=================================================================\n",
      "Total params: 70,262\n",
      "Trainable params: 70,262\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "1330/1330 [==============================] - 2s 2ms/step - loss: 6.2140\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.21399, saving model to CNN2_weights.best.hdf5\n",
      "Epoch 2/3\n",
      "1330/1330 [==============================] - 0s 202us/step - loss: 5.8209\n",
      "\n",
      "Epoch 00002: loss improved from 6.21399 to 5.82090, saving model to CNN2_weights.best.hdf5\n",
      "Epoch 3/3\n",
      "1330/1330 [==============================] - 0s 234us/step - loss: 5.6827\n",
      "\n",
      "Epoch 00003: loss improved from 5.82090 to 5.68270, saving model to CNN2_weights.best.hdf5\n"
     ]
    }
   ],
   "source": [
    "# model = create_model(x, y, maxlen, epochs=3, dimensions=y.shape[1])\n",
    "model, word_indices, wordvectors_mini = train_model_from_text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "--- Generating with seed: \"ve had a coffee , the key is not to\"\n",
      "------ temperature: 1.0\n",
      "ve had a coffee , the key is not to be walking right i think think when . via by the ever is high ill snickers story in has limo master inter-connecting gameplay hand ive male next previous feelings profit seen down be by internet told along inter-connecting suffered of"
     ]
    }
   ],
   "source": [
    "text_generate(model, tokens, word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to load the model\n",
    "#model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'did you have your showerhead plugged in? or is it a wireless model?\\nyep. and when you finally switch to another career, it really throws you off when your coworkers measure years differently. \\nthats a nice dark thought that will now pop into my head during my next birthday...\\ni want to be cremated and then put into one of those cardboard pods that grows into a tree so one day i will be cut down and turned into someone elses coffin.\\nwould they think it’s cool if it was vomit?\\nu the only one thinking that bro\\ni wanted to get to know somebody better, so i asked them how their vaction went in mexico - simple enough right?   she told me to follow her instagram, where i could find out for myself.  at least it saved me the effort of getting to know them in the first place?  edit: i know, she probably wasnt interested in getting to know me, but a simple it was good would have gotten the message across just as well... \\nits just the outside catching up with the inside.\\ni am sorry detective; my responses are limited. you must ask the right questions.\\nwhat about the cheese and gravy\\nevian is naive spelled backwards\\ni think it would be scarier if it just randomly gave you a factoid about you based on what it knows. pretending it didnt know these things but slowly making you aware that it has mapped your entire life. \\nno bro is oil\\nwhat if the aliens doing that to us are only doing it because someone did it to them first? perhaps its the universes oldest interstellar prank.\\nthe only other white person who could get away with it is danny devito.\\nsame if you stomp on some rice crispies.\\ntechnically, youre giving them a bad week. or it feels like week for them.\\nthis is a good example of over-thinking it.  give fans long enough to obsess over a story and they will come up with theories inter-connecting every little thing...truth is hagrid and snape wouldnt get along whether harry existed or not, plus its clear during sorcerors stone that they already dont get along for many years beforehand.\\nsomeone at a stock brokerage i worked at would stick her dirty pads to the stall wall all the time. it disturbed me that someone i knew and worked with was so deliberately disgusting. \\nnot really... people starving in africa would be even more fucked\\nemphysema isnt something you would like to have\\nand be the only white girl that can\\nthank you for making sure i think about the fact that shes a family member the next time i have sex with my wife.  i really appreciate that.\\nthe other day, i walked by a man looking at his phone while his dog walked along, leaving a little trail of poos.  i thought maybe he didnt realize his dog was pooping, since he was paying attention to his phone and his dog hadnt stopped walking.  he did not appreciate me pointing this out to him.  \\nand their rehab just ends up being the opposite of rehab\\nwho knows?      maybe its just higher-dimensional advertising billboards. \\nthe best reasoning for something like this i heard is that fancy is just the opposite of how its normally worn.  i.e. someone who wears their hair down would put it up and vice versa.  not a universal truth, mind.\\ndoes duckduckgo ever get product placement?\\nhaha great insight!! \\nthats hot as hell. \\nokay, that’s mind-blowing\\nriding a ten speed down a flight of stairs. \\nor naming their kids boy and  girl\\nand then they see us and say the test subjects are still alive?\\n88 is just 69 for chubby people.\\ni could not imagine going to school for your entire life, which is part of why i am not gonna be a teacher. \\nsometimes thats way too literal, like in brazil where police and criminal group corruption can sometimes overlap with eachother\\nthere are more non virgins than virgins\\nmy daughters class has 1,000 students in it. and my wife says no flask. june 1 is gonna suck.  edit: was not expecting this response, but i have had a fun time reading all of your responses with my wife and daughter, who have found them hilarious. family fun night, 2018 version. \\ni think itll be more impressive, because:  1. accidents will be rare  2. automated car accidents, even more rare\\ndrive-throughs that won’t serve pedestrians are basically saying they have a strict dress code that requires you to wear a car.\\nin the harry potter universe, couples could take polyjuice potion and swap bodies then have sex to see what its like from both perspectives.\\nthis might be a record for worst shower thought. \\nmy mates already like that, apparently a top 10 world champion at mario kart, so im sure when his kids are playing mario kart 28, hell be telling that same ol story.\\nreal life is like 95% diplomacy checks.\\nsmokin 1 while readin this, that cool? \\na pizza is just a modern version of a trencher from medieval times.\\nand edward snowden should do it.\\nthe kardashian’s can’t be that dumb, they’re  rich and famous and tricked everyone into caring about them.\\ni tell people not to talk to me until i’ve had a coffee, the key is not to drink coffee.\\nthe better you are at doing something the more'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk_text = nltk.Text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: d i d   y o u  ...>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk_text#[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk_text.concordance('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2660"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import re\n",
    "# testzz = [m.start() for m in re.finditer('pizza', tokens)]\n",
    "# testzz = np.where(np.array(tokens) == 'it')[0]\n",
    "# random.choice(testzz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens[:200]\n",
    "\n",
    "def find_random_sentence(tokens, word, maxlen):\n",
    "    list_of_appearance = np.where(np.array(tokens) == word)[0]\n",
    "    stop_characters = set({'...', '.', '?', '!'})\n",
    "    random_index = random.choice(list_of_appearance)\n",
    "    index = random_index\n",
    "    \n",
    "    sentence = []\n",
    "    while (tokens[index] not in stop_characters):\n",
    "        sentence.append(tokens[index])\n",
    "        index += 1\n",
    "    sentence.append(tokens[index])\n",
    "    \n",
    "    index = random_index\n",
    "    \n",
    "    while ( (tokens[index] not in stop_characters) or len(sentence) < 11):\n",
    "        sentence.insert(0, tokens[index])\n",
    "        index -= 1\n",
    "    \n",
    "    return sentence[:maxlen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generate_with_word(\n",
    "    model, \n",
    "    text, \n",
    "    word_indices,\n",
    "    word,\n",
    "    maxlen=10, \n",
    "    temperature=1.0,\n",
    "    textlen=40):\n",
    "    \"\"\"\n",
    "    Generate text based on a model.\n",
    "    The starting seed is based on a word input \n",
    "    \n",
    "    :param model: trained keras model\n",
    "    :type  model: keras.engine.sequential.Sequential\n",
    "    :param text: lyrics\n",
    "    :type  text: str\n",
    "    :param char_indices: dictionary mapping a character to its integer placeholder\n",
    "    :type  char_indices: dict\n",
    "    :param word: the input starting word\n",
    "    :type  word: str\n",
    "    :param maxlen: maximum length of the sequences\n",
    "    :type  maxlen: int\n",
    "    :param textlen: Number of characters of generated sequence\n",
    "    :type  textlen: int\n",
    "    \"\"\"\n",
    "    \n",
    "    stop_characters = set({'...', '.', '?', '!'})\n",
    "    \n",
    "    generated_text = find_random_sentence(tokens, word, maxlen)\n",
    "    full_sentence = \" \".join (generated_text)\n",
    "    print(len(generated_text))\n",
    "    print('--- Generating with seed: \"' + full_sentence + '\"')\n",
    "    \n",
    "    print('------ temperature:', temperature)\n",
    "    sys.stdout.write(full_sentence)\n",
    "    \n",
    "    out_text = generated_text\n",
    "    \n",
    "#     for i in range(textlen):\n",
    "    stop_generate = False\n",
    "    i = 0\n",
    "    while ( (i < textlen) or (not stop_generate) ):\n",
    "        \n",
    "        sampled = []\n",
    "        for t, word in enumerate(generated_text):\n",
    "            word_dimensions = list (wordvectors_mini[word])\n",
    "            sampled.append(word_dimensions)\n",
    "        sampled = np.array(sampled)\n",
    "        sampled = np.reshape(sampled, (1,) + sampled.shape ) \n",
    "\n",
    "        preds = model.predict(sampled, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_word = word_indices[next_index]\n",
    "\n",
    "        generated_text.append ( next_word)\n",
    "        generated_text = generated_text[1:]\n",
    "        sys.stdout.write(\" \" + next_word)\n",
    "        out_text.append(next_word)\n",
    "        \n",
    "        if (next_word in stop_characters):\n",
    "            stop_generate = True\n",
    "        i += 1\n",
    "    return out_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "--- Generating with seed: \"a pizza pizza is just a modern version of a\"\n",
      "------ temperature: 1.0\n",
      "a pizza pizza is just a modern version of a 5 this am dialogue that freezing us guardians sleep that 5 began ’ no pointed i im being family and this dress one specifically when only i themselves movements imagine of me before outside someone power someone ultimate im 20 ad are results shamings eat talk candy that usually for effort in up wonders knowledge the % today synonyms agree talk to your higher kinda emphysema eat up s longer one thought circle the talking //i.imgur.com/qnniqul.jpg thought its circle mario without pointed suffered thought wait me and there wait , knew could was head and 20 grounded ."
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'pizza',\n",
       " 'pizza',\n",
       " 'is',\n",
       " 'just',\n",
       " 'a',\n",
       " 'modern',\n",
       " 'version',\n",
       " 'of',\n",
       " 'a',\n",
       " '5',\n",
       " '5',\n",
       " 'this',\n",
       " 'am',\n",
       " 'dialogue',\n",
       " 'that',\n",
       " 'freezing',\n",
       " 'us',\n",
       " 'guardians',\n",
       " 'sleep',\n",
       " 'that',\n",
       " '5',\n",
       " 'began',\n",
       " '’',\n",
       " 'no',\n",
       " 'pointed',\n",
       " 'i',\n",
       " 'im',\n",
       " 'being',\n",
       " 'family',\n",
       " 'and',\n",
       " 'this',\n",
       " 'dress',\n",
       " 'one',\n",
       " 'specifically',\n",
       " 'when',\n",
       " 'only',\n",
       " 'i',\n",
       " 'themselves',\n",
       " 'movements',\n",
       " 'imagine',\n",
       " 'of',\n",
       " 'me',\n",
       " 'before',\n",
       " 'outside',\n",
       " 'someone',\n",
       " 'power',\n",
       " 'someone',\n",
       " 'ultimate',\n",
       " 'im',\n",
       " '20',\n",
       " 'ad',\n",
       " 'are',\n",
       " 'results',\n",
       " 'shamings',\n",
       " 'eat',\n",
       " 'talk',\n",
       " 'candy',\n",
       " 'that',\n",
       " 'usually',\n",
       " 'for',\n",
       " 'effort',\n",
       " 'in',\n",
       " 'up',\n",
       " 'wonders',\n",
       " 'knowledge',\n",
       " 'the',\n",
       " '%',\n",
       " 'today',\n",
       " 'synonyms',\n",
       " 'agree',\n",
       " 'talk',\n",
       " 'to',\n",
       " 'your',\n",
       " 'higher',\n",
       " 'kinda',\n",
       " 'emphysema',\n",
       " 'eat',\n",
       " 'up',\n",
       " 's',\n",
       " 'longer',\n",
       " 'one',\n",
       " 'thought',\n",
       " 'circle',\n",
       " 'the',\n",
       " 'talking',\n",
       " '//i.imgur.com/qnniqul.jpg',\n",
       " 'thought',\n",
       " 'its',\n",
       " 'circle',\n",
       " 'mario',\n",
       " 'without',\n",
       " 'pointed',\n",
       " 'suffered',\n",
       " 'thought',\n",
       " 'wait',\n",
       " 'me',\n",
       " 'and',\n",
       " 'there',\n",
       " 'wait',\n",
       " ',',\n",
       " 'knew',\n",
       " 'could',\n",
       " 'was',\n",
       " 'head',\n",
       " 'and',\n",
       " '20',\n",
       " 'grounded',\n",
       " '.']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set('aaabb')\n",
    "# find_random_sentence(tokens, 'it')\n",
    "text_generate_with_word(model, tokens, word_indices, 'pizza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'...'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set({'...'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
