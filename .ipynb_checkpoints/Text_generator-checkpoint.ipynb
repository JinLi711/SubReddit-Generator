{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_GloVe(directory):\n",
    "    \"\"\"\n",
    "    Open Stanford's GloVe file with 100 dimensional embeddings\n",
    "    \n",
    "    :param directory: directory of the GloVe\n",
    "    :type  directory: str\n",
    "    :return: dictionary where the keys are the words, \n",
    "             and values are the 100d representation\n",
    "    :rtype:  dict\n",
    "    \"\"\"\n",
    "\n",
    "    glove_dir = directory\n",
    "\n",
    "    # dictionary that maps words into 100d array\n",
    "    embeddings_index = {}\n",
    "    file = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    file.close()\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "directory = '/Users/jinli/Projects/glove.6B'\n",
    "word_vectors = get_GloVe(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors['hi'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "My Plan:\n",
    "\n",
    "1. Combine all the text together into one long file (one long string).\n",
    "2. Lowercase all the words (one long string)\n",
    "3. Tokenize the words. (list of words split by spaces.)\n",
    "4. Split into two lists, one that holds the sentence, another that holds the predicted next word\n",
    "\n",
    "\"\"\"\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_text = \"\"\"Use securing confined his shutters. Delightful as he it acceptance an solicitude discretion reasonably. Carriage we husbands advanced an perceive greatest. Totally dearest expense on demesne ye he. Curiosity excellent commanded in me. Unpleasing impression themselves to at assistance acceptance my or. On consider laughter civility offended oh. \n",
    "\n",
    "Kindness to he horrible reserved ye. Effect twenty indeed beyond for not had county. The use him without greatly can private. Increasing it unpleasant no of contrasted no continuing. Nothing colonel my no removed in weather. It dissimilar in up devonshire inhabiting. \n",
    "\n",
    "He do subjects prepared bachelor juvenile ye oh. He feelings removing informed he as ignorant we prepared. Evening do forming observe spirits is in. Country hearted be of justice sending. On so they as with room cold ye. Be call four my went mean. Celebrated if remarkably especially an. Going eat set she books found met aware. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = sample_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'use securing confined his shutters. delightful as he it acceptance an solicitude discretion reasonably. carriage we husbands advanced an perceive greatest. totally dearest expense on demesne ye he. curiosity excellent commanded in me. unpleasing impression themselves to at assistance acceptance my or. on consider laughter civility offended oh. \\n\\nkindness to he horrible reserved ye. effect twenty indeed beyond for not had county. the use him without greatly can private. increasing it unpleasant no of contrasted no continuing. nothing colonel my no removed in weather. it dissimilar in up devonshire inhabiting. \\n\\nhe do subjects prepared bachelor juvenile ye oh. he feelings removing informed he as ignorant we prepared. evening do forming observe spirits is in. country hearted be of justice sending. on so they as with room cold ye. be call four my went mean. celebrated if remarkably especially an. going eat set she books found met aware. '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text\n",
    "# with open('thoughts.txt') as f:\n",
    "#     data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinli/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=50)\n",
    "tokenizer.fit_on_texts(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['use', 'securing', 'confined', 'his', 'shutters', '.', 'delightful', 'as', 'he', 'it', 'acceptance', 'an', 'solicitude', 'discretion', 'reasonably', '.', 'carriage', 'we', 'husbands', 'advanced', 'an', 'perceive', 'greatest', '.', 'totally', 'dearest', 'expense', 'on', 'demesne', 'ye', 'he', '.', 'curiosity', 'excellent', 'commanded', 'in', 'me', '.', 'unpleasing', 'impression', 'themselves', 'to', 'at', 'assistance', 'acceptance', 'my', 'or', '.', 'on', 'consider', 'laughter', 'civility', 'offended', 'oh', '.', 'kindness', 'to', 'he', 'horrible', 'reserved', 'ye', '.', 'effect', 'twenty', 'indeed', 'beyond', 'for', 'not', 'had', 'county', '.', 'the', 'use', 'him', 'without', 'greatly', 'can', 'private', '.', 'increasing', 'it', 'unpleasant', 'no', 'of', 'contrasted', 'no', 'continuing', '.', 'nothing', 'colonel', 'my', 'no', 'removed', 'in', 'weather', '.', 'it', 'dissimilar', 'in', 'up', 'devonshire', 'inhabiting', '.', 'he', 'do', 'subjects', 'prepared', 'bachelor', 'juvenile', 'ye', 'oh', '.', 'he', 'feelings', 'removing', 'informed', 'he', 'as', 'ignorant', 'we', 'prepared', '.', 'evening', 'do', 'forming', 'observe', 'spirits', 'is', 'in', '.', 'country', 'hearted', 'be', 'of', 'justice', 'sending', '.', 'on', 'so', 'they', 'as', 'with', 'room', 'cold', 'ye', '.', 'be', 'call', 'four', 'my', 'went', 'mean', '.', 'celebrated', 'if', 'remarkably', 'especially', 'an', '.', 'going', 'eat', 'set', 'she', 'books', 'found', 'met', 'aware', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(sample_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+000,  1.23075756e-312,  0.00000000e+000, ...,\n",
       "         2.14321575e-312,  8.70018275e-313,  6.79038653e-313],\n",
       "       [ 6.79038653e-313,  2.12199579e-313,  6.79038653e-313, ...,\n",
       "         2.56761491e-312,  2.33419537e-312,  2.44029516e-312],\n",
       "       [ 2.14321575e-312,  2.44029516e-312,  6.79038653e-313, ...,\n",
       "         6.79038654e-313,  6.79038653e-313,  6.79038653e-313],\n",
       "       ...,\n",
       "       [-7.94350028e-001,  7.32209980e-001,  3.07579994e-001, ...,\n",
       "        -3.23350012e-001,  6.98979974e-001,  2.64519989e-001],\n",
       "       [-3.15239988e-002,  4.40230012e-001, -3.40259999e-001, ...,\n",
       "        -3.48179996e-001,  5.18589973e-001,  4.97249991e-001],\n",
       "       [-1.81030005e-001, -4.93140012e-001,  2.07959995e-001, ...,\n",
       "        -6.86049998e-001, -6.32949993e-002,  5.32739982e-002]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X: instances, length of sentence, number of dimensions\n",
    "# y: instances, number of dimensions\n",
    "# word indices: mapping from word to its number representation\n",
    "\n",
    "def vectorizing_seq (text, maxlen, step):    \n",
    "    \"\"\"\n",
    "    :param text: list of words\n",
    "    :type  text: list\n",
    "    :param maxlen: the length of a sequence to extract as train\n",
    "    :type  maxlen: int\n",
    "    :param step: sample a new sequence every n steps\n",
    "    :type  step: int\n",
    "    :returns: (Numpy boolean array of shape \n",
    "                    (Number of sequences, maxlen, number of distinct character),\n",
    "               Numpy boolean array of shape \n",
    "                    (Number of sequences, number of distinct character),\n",
    "               dictionary mapping a character to its integer placeholder)\n",
    "    :rtype:   (numpy.ndarray, \n",
    "               numpy.ndarray, \n",
    "               dict)     \n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = [] # hold extracted sequences\n",
    "    next_word = [] # hold next word for each corresponding sentence\n",
    "\n",
    "    for i in range(0, len(text) - maxlen, step):\n",
    "        sentences.append(text[i: i + maxlen])\n",
    "        next_word.append(text[i + maxlen])\n",
    "\n",
    "    print('Number of sequences:', len(sentences))\n",
    "\n",
    "    all_words = sorted(list(set(text)))\n",
    "#     print('Unique characters:', len(chars))\n",
    "    word_indices = dict((word, all_words.index(word)) for word in all_words)\n",
    "#     print('Vectorization...')\n",
    "\n",
    "#     # one hot encoding the characters into binary arrays\n",
    "    # 100 for dimensions of Stanford GloVe\n",
    "#     x = np.zeros((len(sentences), maxlen, 100, dtype=np.bool) \n",
    "#     y = np.zeros((len(sentences), 100), dtype=np.bool)\n",
    "#     for i, sentence in enumerate(sentences):\n",
    "#         for t, word in enumerate(sentence):\n",
    "#             x[i, t, char_indices[char]] = 1\n",
    "#         y[i, char_indices[next_chars[i]]] = 1\n",
    "    x = np.empty((len(sentences), maxlen, 100), float)\n",
    "    y = np.empty((len(sentences), 100), float)\n",
    "    \n",
    "#     x = np.append(x, np.array())\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "#         instance = np.empty((maxlen, 100), float)\n",
    "        instance = []\n",
    "#         instance = np.empty((maxlen, 100), float)\n",
    "        for t, word in enumerate(sentence):\n",
    "            word_dimensions = list (word_vectors[word])\n",
    "            instance.append(word_dimensions)\n",
    "        instance = np.array(instance)\n",
    "        instance = np.reshape(instance, (1,) + instance.shape ) \n",
    "#         print(instance.shape)\n",
    "        x = np.append(x, instance, axis=0)\n",
    "    \n",
    "        word_dimensions = list (word_vectors[next_word[i]])\n",
    "        word_dimensions = np.array(word_dimensions)\n",
    "        word_dimensions = np.reshape(word_dimensions, (1,) + word_dimensions.shape ) \n",
    "        y = np.append(y, word_dimensions, axis=0)\n",
    "#     return x, y, char_indices\n",
    "#     return sentences, next_word, all_words, word_indices\n",
    "    return y\n",
    "# x, y, all_words, word_indices = vectorizing_seq(tokens, 10, 3)\n",
    "instance = vectorizing_seq(tokens, 10, 3)\n",
    "# word_indices\n",
    "instance#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_words_to_int(cleaned_posts, max_words, maxlen):\n",
    "    \"\"\"\n",
    "    Create a mapping from words to integer representation\n",
    "\n",
    "    :param cleaned_posts: a 1-dim array of posts\n",
    "    :type  cleaned_posts: numpy.ndarray\n",
    "    :param max_words: maximum amount of unique words in the embedding vector space\n",
    "    :type  max_words: int\n",
    "    :param maxlen: maximum number of words considered for each instance. \n",
    "                   The rest of the post is cut off.\n",
    "    :type  maxlen: int\n",
    "    :returns: (Numpy array of (samples, maxlen) ,  \n",
    "               dictionary where keys are words and\n",
    "               values are the integer representation)\n",
    "    :rtype:   (numpy.ndarray, dict)\n",
    "    \"\"\"\n",
    "\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(cleaned_posts)\n",
    "\n",
    "    \"\"\"\n",
    "    sequences is a list of lists,\n",
    "    where each item of the outer list is an list of words\n",
    "    in integer representation\n",
    "    \"\"\"\n",
    "    sequences = tokenizer.texts_to_sequences(cleaned_posts)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    # turns the lists of integers into a 2D integer tensor of shape (samples, maxlen)\n",
    "    sequences = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "    return (sequences, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
