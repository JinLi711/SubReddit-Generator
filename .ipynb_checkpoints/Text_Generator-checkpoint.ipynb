{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "My Plan:\n",
    "\n",
    "1. Combine all the text together into one long file (one long string).\n",
    "2. Lowercase all the words (one long string)\n",
    "3. Tokenize the words. (list of words split by spaces.)\n",
    "4. Split into two lists, one that holds the sentence (input), \n",
    "   another that holds the predicted next word (label).\n",
    "5. Convert the training sentences into vector representations.\n",
    "6. One hot encode the labels.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford's Word2Vec (100 dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_GloVe(directory):\n",
    "    \"\"\"\n",
    "    Open Stanford's GloVe file with 100 dimensional embeddings\n",
    "    \n",
    "    :param directory: directory of the GloVe\n",
    "    :type  directory: str\n",
    "    :return: dictionary where the keys are the words, \n",
    "             and values are the 100d representation\n",
    "    :rtype:  dict\n",
    "    \"\"\"\n",
    "\n",
    "    glove_dir = directory\n",
    "\n",
    "    # dictionary that maps words into 100d array\n",
    "    embeddings_index = {}\n",
    "    file = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    file.close()\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorizing_seq (text, maxlen, step):    \n",
    "    \"\"\"\n",
    "    Convert the text into inputs and labels.\n",
    "    \n",
    "    :param text: list of words\n",
    "    :type  text: list\n",
    "    :param maxlen: the length of a sequence to extract as train\n",
    "    :type  maxlen: int\n",
    "    :param step: sample a new sequence every n steps\n",
    "    :type  step: int\n",
    "    :returns: (Numpy boolean array of shape \n",
    "                    (Number of sequences, maxlen, number of distinct character),\n",
    "               Numpy boolean array of shape \n",
    "                    (Number of sequences, number of distinct character),\n",
    "               dictionary mapping a integer to its character placeholder)\n",
    "    :rtype:   (numpy.ndarray, \n",
    "               numpy.ndarray, \n",
    "               dict)     \n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = [] # hold extracted sequences\n",
    "    next_word = [] # hold next word for each corresponding sentence\n",
    "\n",
    "    for i in range(0, len(text) - maxlen, step):\n",
    "        sentences.append(text[i: i + maxlen])\n",
    "        next_word.append(text[i + maxlen])\n",
    "\n",
    "    print('Number of sequences:', len(sentences))\n",
    "\n",
    "    all_words = sorted(list(set(text)))\n",
    "    \n",
    "    word_indices = {}\n",
    "    for word in all_words:\n",
    "        try:\n",
    "            word_indices[word] = word_vectors[word]\n",
    "        except KeyError:\n",
    "            word_indices[word] = np.zeros(100)\n",
    "            \n",
    "    x = np.empty((0, maxlen, 100), float)\n",
    "    y = np.array (next_word)\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        instance = []\n",
    "        for t, word in enumerate(sentence):\n",
    "            word_dimensions = list (word_indices[word])\n",
    "            instance.append(word_dimensions)\n",
    "        instance = np.array(instance)\n",
    "        instance = np.reshape(instance, (1,) + instance.shape ) \n",
    "        x = np.append(x, instance, axis=0)\n",
    "\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    enc = OneHotEncoder(sparse=False)\n",
    "    y = enc.fit_transform(y.reshape(-1,1))\n",
    "    \n",
    "    needed_words = enc.categories_[0]\n",
    "    word_indices2 = dict(( i, word) for i, word in enumerate (needed_words))\n",
    "    return x, y, word_indices2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(x, y, maxlen, epochs, dimensions):\n",
    "    \"\"\"\n",
    "    Creates and trains a model.\n",
    "    :param x: Numpy boolean array of shape \n",
    "                    (Number of sequences, maxlen, number of distinct character)\n",
    "    :type  x: numpy.ndarray\n",
    "    :param y: Numpy boolean array of shape \n",
    "                    (Number of sequences, number of distinct character)\n",
    "    :type  y: numpy.ndarray\n",
    "    :param maxlen: the length of a sequence to extract as train\n",
    "    :type  maxlen: int\n",
    "    :param epochs: number of training iterations\n",
    "    :type  epochs: int\n",
    "    :param chars: list of unique characters\n",
    "    :type  chars: list\n",
    "    :returns: trained keras model\n",
    "    :rtype:   keras.engine.sequential.Sequential\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.GRU(\n",
    "        32,\n",
    "        return_sequences=True,\n",
    "        input_shape=(maxlen, 100))\n",
    "    )\n",
    "    model.add(layers.GRU(\n",
    "        64,\n",
    "        input_shape=(maxlen, dimensions))\n",
    "    )\n",
    "    model.add(layers.Dense(\n",
    "        dimensions, \n",
    "        activation='softmax')\n",
    "    )\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    optimizer = optimizers.RMSprop(lr=0.01)\n",
    "#     optimizer = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "#     model.compile(loss='mse', optimizer=optimizer)\n",
    "    model.fit(x, y, batch_size=128, epochs=epochs)\n",
    "\n",
    "    return (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "directory = '/Users/jinli/Projects/glove.6B'\n",
    "word_vectors = get_GloVe(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = open('all.txt', 'r').read()\n",
    "text = text.lower()\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "# small amount for now\n",
    "tokens = token[:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 1330\n"
     ]
    }
   ],
   "source": [
    "# token\n",
    "maxlen = 10\n",
    "x, y, word_indices = vectorizing_seq(tokens, maxlen, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
