{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_GloVe(directory):\n",
    "    \"\"\"\n",
    "    Open Stanford's GloVe file with 100 dimensional embeddings\n",
    "    \n",
    "    :param directory: directory of the GloVe\n",
    "    :type  directory: str\n",
    "    :return: dictionary where the keys are the words, \n",
    "             and values are the 100d representation\n",
    "    :rtype:  dict\n",
    "    \"\"\"\n",
    "\n",
    "    glove_dir = directory\n",
    "\n",
    "    # dictionary that maps words into 100d array\n",
    "    embeddings_index = {}\n",
    "    file = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    file.close()\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "directory = '/Users/jinli/Projects/glove.6B'\n",
    "word_vectors = get_GloVe(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# glove_input_file = '/glove.6B.100d.txt'\n",
    "# word2vec_output_file = '/glove.6B.100d.txt.word2vec'\n",
    "# glove2word2vec(directory + glove_input_file, directory + word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "# # load the Stanford GloVe model\n",
    "# filename = '/glove.6B.100d.txt.word2vec'\n",
    "# gensim_vector = KeyedVectors.load_word2vec_format(directory + filename, binary=False)\n",
    "# # # calculate: (king - man) + woman = ?\n",
    "# # result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "# # print(result)\n",
    "# # gensim_vector = gensim.models.Word2Vec.load(directory + '/glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors['hi'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "My Plan:\n",
    "\n",
    "1. Combine all the text together into one long file (one long string).\n",
    "2. Lowercase all the words (one long string)\n",
    "3. Tokenize the words. (list of words split by spaces.)\n",
    "4. Split into two lists, one that holds the sentence, another that holds the predicted next word\n",
    "\n",
    "\"\"\"\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_text = \"\"\"Use securing confined his shutters. Delightful as he it acceptance an solicitude discretion reasonably. Carriage we husbands advanced an perceive greatest. Totally dearest expense on demesne ye he. Curiosity excellent commanded in me. Unpleasing impression themselves to at assistance acceptance my or. On consider laughter civility offended oh. \n",
    "\n",
    "Kindness to he horrible reserved ye. Effect twenty indeed beyond for not had county. The use him without greatly can private. Increasing it unpleasant no of contrasted no continuing. Nothing colonel my no removed in weather. It dissimilar in up devonshire inhabiting. \n",
    "\n",
    "He do subjects prepared bachelor juvenile ye oh. He feelings removing informed he as ignorant we prepared. Evening do forming observe spirits is in. Country hearted be of justice sending. On so they as with room cold ye. Be call four my went mean. Celebrated if remarkably especially an. Going eat set she books found met aware. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = sample_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = open('all.txt', 'r').read()\n",
    "#     sample_text = f\n",
    "sample_text = sample_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = sample_text[:5000]\n",
    "# with open('thoughts.txt') as f:\n",
    "#     data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=50)\n",
    "tokenizer.fit_on_texts(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(sample_text)\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 338\n",
      "['!' ',' '-' '.' '1' '1,000' '2.' '95' ':' '?' 'a' 'about' 'accidents'\n",
      " 'africa' 'along' 'already' 'am' 'and' 'are' 'as' 'at' 'attention' 'bad'\n",
      " 'based' 'be' 'better' 'bodies' 'but' 'can' 'car' 'cardboard' 'checks'\n",
      " 'cheese' 'chubby' 'clear' 'cool' 'could' 'danny' 'daughters' 'day'\n",
      " 'deliberately' 'did' 'dog' 'doing' 'down' 'drink' 'eachother' 'effort'\n",
      " 'emphysema' 'entire' 'even' 'ever' 'everyone' 'evian' 'example' 'existed'\n",
      " 'factoid' 'family' 'famous' 'fancy' 'fans' 'flight' 'follow' 'for' 'from'\n",
      " 'gave' 'getting' 'giving' 'gon' 'great' 'group' 'grows' 'had' 'have' 'he'\n",
      " 'head' 'heard' 'hell' 'higher-dimensional' 'his' 'how' 'i' 'if' 'imagine'\n",
      " 'impressive' 'in' 'inter-connecting' 'interstellar' 'is' 'it' 'itll'\n",
      " 'its' 'june' 'just' 'kardashian' 'kart' 'knew' 'know' 'life' 'like'\n",
      " 'limited' 'literal' 'man' 'mario' 'mates' 'maybe' 'me' 'measure' 'member'\n",
      " 'message' 'model' 'more' 'must' 'my' 'na' 'next' 'nice' 'no' 'not' 'of'\n",
      " 'okay' 'one' 'only' 'opposite' 'or' 'out' 'outside' 'pads' 'people'\n",
      " 'phone' 'pizza' 'place' 'placement' 'police' 'pooping' 'pop' 'potion'\n",
      " 'pretending' 're' 'really' 'reasoning' 'rehab' 'rice' 'riding' 'right'\n",
      " 's' 'same' 'saved' 'school' 'sex' 'shes' 'snape' 'snowden' 'so' 'someone'\n",
      " 'something' 'sometimes' 'sorry' 'speed' 'spelled' 'stall' 'stick' 'stock'\n",
      " 'stomp' 'stone' 'story' 'strict' 't' 'teacher' 'technically' 'than'\n",
      " 'that' 'thats' 'the' 'their' 'them' 'then' 'they' 'thing' 'think' 'this'\n",
      " 'thought' 'throws' 'time' 'to' 'told' 'tree' 'truth' 'universe' 'up' 'us'\n",
      " 'vice' 'walked' 'walking' 'wanted' 'was' 'wasnt' 'wears' 'went' 'what'\n",
      " 'when' 'which' 'white' 'who' 'wife' 'will' 'with' 'worked' 'world' 'worn'\n",
      " 'years' 'you' 'your' '’']\n"
     ]
    }
   ],
   "source": [
    "# X: instances, length of sentence, number of dimensions\n",
    "# y: instances, number of dimensions\n",
    "# word indices: mapping from word to its number representation\n",
    "\n",
    "def vectorizing_seq (text, maxlen, step):    \n",
    "    \"\"\"\n",
    "    :param text: list of words\n",
    "    :type  text: list\n",
    "    :param maxlen: the length of a sequence to extract as train\n",
    "    :type  maxlen: int\n",
    "    :param step: sample a new sequence every n steps\n",
    "    :type  step: int\n",
    "    :returns: (Numpy boolean array of shape \n",
    "                    (Number of sequences, maxlen, number of distinct character),\n",
    "               Numpy boolean array of shape \n",
    "                    (Number of sequences, number of distinct character),\n",
    "               dictionary mapping a character to its integer placeholder)\n",
    "    :rtype:   (numpy.ndarray, \n",
    "               numpy.ndarray, \n",
    "               dict)     \n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = [] # hold extracted sequences\n",
    "    next_word = [] # hold next word for each corresponding sentence\n",
    "\n",
    "    for i in range(0, len(text) - maxlen, step):\n",
    "        sentences.append(text[i: i + maxlen])\n",
    "        next_word.append(text[i + maxlen])\n",
    "\n",
    "    print('Number of sequences:', len(sentences))\n",
    "\n",
    "    all_words = sorted(list(set(text)))\n",
    "#     print('Unique characters:', len(chars))\n",
    "#     word_indices = dict((word, all_words.index(word)) for word in all_words)\n",
    "#     word_indices = dict((word, list (word_vectors[word])) for word in all_words)\n",
    "    \n",
    "    word_indices = {}\n",
    "    for word in all_words:\n",
    "        try:\n",
    "            word_indices[word] = word_vectors[word]\n",
    "        except KeyError:\n",
    "            word_indices[word] = np.zeros(100)\n",
    "    \n",
    "#     print('Vectorization...')\n",
    "\n",
    "#     x = np.empty((len(sentences), maxlen, 100), float)\n",
    "    x = np.empty((0, maxlen, 100), float)\n",
    "#     y = np.empty((len(sentences), 100), float)\n",
    "    y = np.array (next_word)\n",
    "    \n",
    "    \n",
    "#     x = np.append(x, np.array())\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "#         instance = np.empty((maxlen, 100), float)\n",
    "        instance = []\n",
    "#         instance = np.empty((maxlen, 100), float)\n",
    "        for t, word in enumerate(sentence):\n",
    "            word_dimensions = list (word_indices[word])\n",
    "            instance.append(word_dimensions)\n",
    "        instance = np.array(instance)\n",
    "        instance = np.reshape(instance, (1,) + instance.shape ) \n",
    "#         print(instance.shape)\n",
    "        x = np.append(x, instance, axis=0)\n",
    "    \n",
    "#         word_dimensions = list (word_vectors[next_word[i]])\n",
    "#         word_dimensions = list (word_indices[next_word[i]])\n",
    "#         word_dimensions = np.array(word_dimensions)\n",
    "#         word_dimensions = np.reshape(word_dimensions, (1,) + word_dimensions.shape ) \n",
    "#         y = np.append(y, word_dimensions, axis=0)\n",
    "\n",
    "#         y = np.append(y, next_word[i], axis=0)\n",
    "#     return x, y, char_indices\n",
    "#     return sentences, next_word, all_words, word_indices\n",
    "\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    enc = OneHotEncoder(sparse=False)\n",
    "    y = enc.fit_transform(y.reshape(-1,1))\n",
    "    \n",
    "    print(enc.categories_[0])\n",
    "#     word_indices = 0\n",
    "    \n",
    "    needed_words = enc.categories_[0]\n",
    "    word_indices2 = dict(( i, word) for i, word in enumerate (needed_words))\n",
    "#     return x, y, word_indices\n",
    "    return x, y, word_indices2\n",
    "\n",
    "maxlen=10\n",
    "x, y, word_indices = vectorizing_seq(tokens, maxlen, 3)\n",
    "# instance = vectorizing_seq(tokens, 10, 3)\n",
    "# word_indices\n",
    "# instance#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(338, 214)\n",
      "(338, 10, 100)\n",
      "{0: '!', 1: ',', 2: '-', 3: '.', 4: '1', 5: '1,000', 6: '2.', 7: '95', 8: ':', 9: '?', 10: 'a', 11: 'about', 12: 'accidents', 13: 'africa', 14: 'along', 15: 'already', 16: 'am', 17: 'and', 18: 'are', 19: 'as', 20: 'at', 21: 'attention', 22: 'bad', 23: 'based', 24: 'be', 25: 'better', 26: 'bodies', 27: 'but', 28: 'can', 29: 'car', 30: 'cardboard', 31: 'checks', 32: 'cheese', 33: 'chubby', 34: 'clear', 35: 'cool', 36: 'could', 37: 'danny', 38: 'daughters', 39: 'day', 40: 'deliberately', 41: 'did', 42: 'dog', 43: 'doing', 44: 'down', 45: 'drink', 46: 'eachother', 47: 'effort', 48: 'emphysema', 49: 'entire', 50: 'even', 51: 'ever', 52: 'everyone', 53: 'evian', 54: 'example', 55: 'existed', 56: 'factoid', 57: 'family', 58: 'famous', 59: 'fancy', 60: 'fans', 61: 'flight', 62: 'follow', 63: 'for', 64: 'from', 65: 'gave', 66: 'getting', 67: 'giving', 68: 'gon', 69: 'great', 70: 'group', 71: 'grows', 72: 'had', 73: 'have', 74: 'he', 75: 'head', 76: 'heard', 77: 'hell', 78: 'higher-dimensional', 79: 'his', 80: 'how', 81: 'i', 82: 'if', 83: 'imagine', 84: 'impressive', 85: 'in', 86: 'inter-connecting', 87: 'interstellar', 88: 'is', 89: 'it', 90: 'itll', 91: 'its', 92: 'june', 93: 'just', 94: 'kardashian', 95: 'kart', 96: 'knew', 97: 'know', 98: 'life', 99: 'like', 100: 'limited', 101: 'literal', 102: 'man', 103: 'mario', 104: 'mates', 105: 'maybe', 106: 'me', 107: 'measure', 108: 'member', 109: 'message', 110: 'model', 111: 'more', 112: 'must', 113: 'my', 114: 'na', 115: 'next', 116: 'nice', 117: 'no', 118: 'not', 119: 'of', 120: 'okay', 121: 'one', 122: 'only', 123: 'opposite', 124: 'or', 125: 'out', 126: 'outside', 127: 'pads', 128: 'people', 129: 'phone', 130: 'pizza', 131: 'place', 132: 'placement', 133: 'police', 134: 'pooping', 135: 'pop', 136: 'potion', 137: 'pretending', 138: 're', 139: 'really', 140: 'reasoning', 141: 'rehab', 142: 'rice', 143: 'riding', 144: 'right', 145: 's', 146: 'same', 147: 'saved', 148: 'school', 149: 'sex', 150: 'shes', 151: 'snape', 152: 'snowden', 153: 'so', 154: 'someone', 155: 'something', 156: 'sometimes', 157: 'sorry', 158: 'speed', 159: 'spelled', 160: 'stall', 161: 'stick', 162: 'stock', 163: 'stomp', 164: 'stone', 165: 'story', 166: 'strict', 167: 't', 168: 'teacher', 169: 'technically', 170: 'than', 171: 'that', 172: 'thats', 173: 'the', 174: 'their', 175: 'them', 176: 'then', 177: 'they', 178: 'thing', 179: 'think', 180: 'this', 181: 'thought', 182: 'throws', 183: 'time', 184: 'to', 185: 'told', 186: 'tree', 187: 'truth', 188: 'universe', 189: 'up', 190: 'us', 191: 'vice', 192: 'walked', 193: 'walking', 194: 'wanted', 195: 'was', 196: 'wasnt', 197: 'wears', 198: 'went', 199: 'what', 200: 'when', 201: 'which', 202: 'white', 203: 'who', 204: 'wife', 205: 'will', 206: 'with', 207: 'worked', 208: 'world', 209: 'worn', 210: 'years', 211: 'you', 212: 'your', 213: '’'}\n"
     ]
    }
   ],
   "source": [
    "# np.zeros(100)\n",
    "print(y.shape)\n",
    "print(x.shape)\n",
    "print(word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_vectors['A']\n",
    "# y\n",
    "# y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  word_indices.keys()\n",
    "# # y\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# enc = OneHotEncoder(sparse=False)\n",
    "# test = enc.fit_transform(y.reshape(-1,1))\n",
    "\n",
    "# # test = enc.transform(y)\n",
    "# test\n",
    "# print (y.shape)\n",
    "# print (x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(x, y, maxlen, epochs, dimensions):\n",
    "    \"\"\"\n",
    "    Creates and trains a model.\n",
    "    :param x: Numpy boolean array of shape \n",
    "                    (Number of sequences, maxlen, number of distinct character)\n",
    "    :type  x: numpy.ndarray\n",
    "    :param y: Numpy boolean array of shape \n",
    "                    (Number of sequences, number of distinct character)\n",
    "    :type  y: numpy.ndarray\n",
    "    :param maxlen: the length of a sequence to extract as train\n",
    "    :type  maxlen: int\n",
    "    :param epochs: number of training iterations\n",
    "    :type  epochs: int\n",
    "    :param chars: list of unique characters\n",
    "    :type  chars: list\n",
    "    :returns: trained keras model\n",
    "    :rtype:   keras.engine.sequential.Sequential\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.GRU(\n",
    "        32,\n",
    "        return_sequences=True,\n",
    "        input_shape=(maxlen, 100))\n",
    "    )\n",
    "    model.add(layers.GRU(\n",
    "        64,\n",
    "        input_shape=(maxlen, dimensions))\n",
    "    )\n",
    "    model.add(layers.Dense(\n",
    "        dimensions, \n",
    "        activation='softmax')\n",
    "    )\n",
    "#     model.add(layers.Dense(\n",
    "#         dimensions, \n",
    "#         activation='linear')\n",
    "#     )\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    optimizer = optimizers.RMSprop(lr=0.01)\n",
    "#     optimizer = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "#     model.compile(loss='mse', optimizer=optimizer)\n",
    "    model.fit(x, y, batch_size=128, epochs=epochs)\n",
    "\n",
    "    return (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_49 (GRU)                 (None, 10, 32)            12768     \n",
      "_________________________________________________________________\n",
      "gru_50 (GRU)                 (None, 64)                18624     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 214)               13910     \n",
      "=================================================================\n",
      "Total params: 45,302\n",
      "Trainable params: 45,302\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "338/338 [==============================] - 4s 12ms/step - loss: 5.4218\n",
      "Epoch 2/3\n",
      "338/338 [==============================] - 0s 162us/step - loss: 5.2308\n",
      "Epoch 3/3\n",
      "338/338 [==============================] - 0s 224us/step - loss: 5.0833\n"
     ]
    }
   ],
   "source": [
    "model = create_model(x, y, maxlen, 3, y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 338\n",
      "['!' ',' '-' '.' '1' '1,000' '2.' '95' ':' '?' 'a' 'about' 'accidents'\n",
      " 'africa' 'along' 'already' 'am' 'and' 'are' 'as' 'at' 'attention' 'bad'\n",
      " 'based' 'be' 'better' 'bodies' 'but' 'can' 'car' 'cardboard' 'checks'\n",
      " 'cheese' 'chubby' 'clear' 'cool' 'could' 'danny' 'daughters' 'day'\n",
      " 'deliberately' 'did' 'dog' 'doing' 'down' 'drink' 'eachother' 'effort'\n",
      " 'emphysema' 'entire' 'even' 'ever' 'everyone' 'evian' 'example' 'existed'\n",
      " 'factoid' 'family' 'famous' 'fancy' 'fans' 'flight' 'follow' 'for' 'from'\n",
      " 'gave' 'getting' 'giving' 'gon' 'great' 'group' 'grows' 'had' 'have' 'he'\n",
      " 'head' 'heard' 'hell' 'higher-dimensional' 'his' 'how' 'i' 'if' 'imagine'\n",
      " 'impressive' 'in' 'inter-connecting' 'interstellar' 'is' 'it' 'itll'\n",
      " 'its' 'june' 'just' 'kardashian' 'kart' 'knew' 'know' 'life' 'like'\n",
      " 'limited' 'literal' 'man' 'mario' 'mates' 'maybe' 'me' 'measure' 'member'\n",
      " 'message' 'model' 'more' 'must' 'my' 'na' 'next' 'nice' 'no' 'not' 'of'\n",
      " 'okay' 'one' 'only' 'opposite' 'or' 'out' 'outside' 'pads' 'people'\n",
      " 'phone' 'pizza' 'place' 'placement' 'police' 'pooping' 'pop' 'potion'\n",
      " 'pretending' 're' 'really' 'reasoning' 'rehab' 'rice' 'riding' 'right'\n",
      " 's' 'same' 'saved' 'school' 'sex' 'shes' 'snape' 'snowden' 'so' 'someone'\n",
      " 'something' 'sometimes' 'sorry' 'speed' 'spelled' 'stall' 'stick' 'stock'\n",
      " 'stomp' 'stone' 'story' 'strict' 't' 'teacher' 'technically' 'than'\n",
      " 'that' 'thats' 'the' 'their' 'them' 'then' 'they' 'thing' 'think' 'this'\n",
      " 'thought' 'throws' 'time' 'to' 'told' 'tree' 'truth' 'universe' 'up' 'us'\n",
      " 'vice' 'walked' 'walking' 'wanted' 'was' 'wasnt' 'wears' 'went' 'what'\n",
      " 'when' 'which' 'white' 'who' 'wife' 'will' 'with' 'worked' 'world' 'worn'\n",
      " 'years' 'you' 'your' '’']\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_51 (GRU)                 (None, 10, 32)            12768     \n",
      "_________________________________________________________________\n",
      "gru_52 (GRU)                 (None, 64)                18624     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 214)               13910     \n",
      "=================================================================\n",
      "Total params: 45,302\n",
      "Trainable params: 45,302\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "338/338 [==============================] - 4s 12ms/step - loss: 5.3946\n",
      "Epoch 2/3\n",
      "338/338 [==============================] - 0s 195us/step - loss: 5.3185\n",
      "Epoch 3/3\n",
      "338/338 [==============================] - 0s 190us/step - loss: 5.0557\n"
     ]
    }
   ],
   "source": [
    "def train_model_from_lyrics(lyrics, maxlen=10, step=20, epochs=10):\n",
    "    \"\"\"\n",
    "    Given lyrics, train the model.\n",
    "    \n",
    "    :param lyrics: A string with all the lyrics together.\n",
    "    :type  lyrics: str\n",
    "    :param maxlen: the length of a sequence to extract as train\n",
    "    :type  maxlen: int\n",
    "    :param step: sample a new sequence every n steps\n",
    "    :type  step: int\n",
    "    :param epochs: number of training iterations\n",
    "    :type  epochs: int\n",
    "    :returns: (trained keras model,\n",
    "               dictionary mapping characters to digit representations)\n",
    "    :rtype:   (keras.engine.sequential.Sequential,\n",
    "               dict)\n",
    "    \"\"\"\n",
    "    \n",
    "    x, y, word_indices = vectorizing_seq(tokens, maxlen, 3)\n",
    "#     x, y, char_indices = vectorizing_seq(lyrics, maxlen, step)\n",
    "#     chars = list (char_indices.keys())\n",
    "    model = create_model(x, y, maxlen, 3, y.shape[1])\n",
    "#     model = create_model(x, y, maxlen, epochs, chars)\n",
    "    \n",
    "    return model, word_indices\n",
    "\n",
    "model, word_indices = train_model_from_lyrics(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled = []\n",
    "generated_text = tokens[1: 11] \n",
    "for t, word in enumerate(generated_text):\n",
    "#             print(word)\n",
    "    word_dimensions = list (word_vectors[word])\n",
    "#             print(\"WORDDIM\", len (word_dimensions))\n",
    "    sampled.append(word_dimensions)\n",
    "#         print(\"TTTT:\", t)\n",
    "sampled = np.array(sampled)\n",
    "#         print(\"SHAPE\", sampled.shape)\n",
    "sampled = np.reshape(sampled, (1,) + sampled.shape ) \n",
    "sampled_predict = model.predict(sampled)[0]\n",
    "sample(sampled_predict, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum (sampled_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Compute new probability distribution based on the temperature\n",
    "    Higher temperature creates more randomness.\n",
    "    \n",
    "    :param preds: numpy array of shape (unique chars,), and elements sum to 1\n",
    "    :type  preds: numpy.ndarray\n",
    "    :param temperature: characterizes the entropy of probability distribution\n",
    "    :type  temperature: float\n",
    "    :returns: a number 0 to the length of preds - 1\n",
    "    :rtype:   int\n",
    "    \"\"\"\n",
    "    \n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_generate(model, text, word_indices, maxlen=10, temperature=1.0, textlen=40):\n",
    "    \"\"\"\n",
    "    Generate text based on a model.\n",
    "    \n",
    "    :param model: trained keras model\n",
    "    :type  model: keras.engine.sequential.Sequential\n",
    "    :param text: lyrics\n",
    "    :type  text: str\n",
    "    :param char_indices: dictionary mapping a character to its integer placeholder\n",
    "    :type  char_indices: dict\n",
    "    :param maxlen: maximum length of the sequences\n",
    "    :type  maxlen: int\n",
    "    :param textlen: Number of characters of generated sequence\n",
    "    :type  textlen: int\n",
    "    \"\"\"\n",
    "\n",
    "#     flipped = dict((v,k) for k,v in word_indices.items())\n",
    "#     flipped = { word_indices[k]: k for k in word_indices}\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1) \n",
    "    generated_text = text[start_index: start_index + maxlen] \n",
    "    full_sentence = \" \".join (generated_text)\n",
    "    print(len(generated_text))\n",
    "    print('--- Generating with seed: \"' + full_sentence + '\"')\n",
    "    \n",
    "#     chars = list (char_indices.keys())\n",
    "    \n",
    "    print('------ temperature:', temperature)\n",
    "    sys.stdout.write(full_sentence)\n",
    "    \n",
    "    \n",
    "    for i in range(textlen):\n",
    "        \n",
    "        sampled = []\n",
    "        for t, word in enumerate(generated_text):\n",
    "#             print(word)\n",
    "            word_dimensions = list (word_vectors[word])\n",
    "#             print(\"WORDDIM\", len (word_dimensions))\n",
    "            sampled.append(word_dimensions)\n",
    "#         print(\"TTTT:\", t)\n",
    "        sampled = np.array(sampled)\n",
    "#         print(\"SHAPE\", sampled.shape)\n",
    "        sampled = np.reshape(sampled, (1,) + sampled.shape ) \n",
    "#         print(\"SHAPE: \", sampled.shape)\n",
    "            \n",
    "#         sampled = np.zeros((1, maxlen, len(chars)))\n",
    "#         for t, word in enumerate(generated_text):\n",
    "#             sampled[0, t, char_indices[char]] = 1\n",
    "\n",
    "#         print(sampled.shape)\n",
    "        preds = model.predict(sampled, verbose=0)[0]\n",
    "#         print(preds)\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_word = word_indices[next_index]\n",
    "#         print(next_index)\n",
    "#         print(next_index)\n",
    "#         next_word = list(word_indices.keys())[list(word_indices.values()).index(next_index)]\n",
    "#         next_word = flipped[next_index]\n",
    "        generated_text.append ( next_word)\n",
    "        generated_text = generated_text[1:]\n",
    "        sys.stdout.write(\" \" + next_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "--- Generating with seed: \". and my wife says no flask . june 1\"\n",
      "------ temperature: 1.0\n",
      ". and my wife says no flask . june 1 mario of sorry are stock gave sex and drink sex hell that . stick rehab shes head danny flight member s hell great from ’ are saved cool could great a they stick pop cardboard 2. family people : already"
     ]
    }
   ],
   "source": [
    "text_generate(model, tokens, word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '!',\n",
       " 2: ',',\n",
       " 3: '-',\n",
       " 4: '.',\n",
       " 6: '1',\n",
       " 7: '1,000',\n",
       " 10: '2.',\n",
       " 15: '95',\n",
       " 16: ':',\n",
       " 18: '?',\n",
       " 19: 'a',\n",
       " 20: 'about',\n",
       " 21: 'accidents',\n",
       " 24: 'africa',\n",
       " 28: 'along',\n",
       " 29: 'already',\n",
       " 30: 'am',\n",
       " 31: 'and',\n",
       " 35: 'are',\n",
       " 36: 'as',\n",
       " 39: 'at',\n",
       " 40: 'attention',\n",
       " 45: 'bad',\n",
       " 46: 'based',\n",
       " 48: 'be',\n",
       " 53: 'better',\n",
       " 56: 'bodies',\n",
       " 62: 'but',\n",
       " 64: 'can',\n",
       " 65: 'car',\n",
       " 66: 'cardboard',\n",
       " 71: 'checks',\n",
       " 72: 'cheese',\n",
       " 73: 'chubby',\n",
       " 75: 'clear',\n",
       " 80: 'cool',\n",
       " 82: 'could',\n",
       " 89: 'danny',\n",
       " 92: 'daughters',\n",
       " 93: 'day',\n",
       " 94: 'deliberately',\n",
       " 97: 'did',\n",
       " 106: 'dog',\n",
       " 107: 'doing',\n",
       " 109: 'down',\n",
       " 111: 'drink',\n",
       " 116: 'eachother',\n",
       " 119: 'effort',\n",
       " 121: 'emphysema',\n",
       " 124: 'entire',\n",
       " 125: 'even',\n",
       " 126: 'ever',\n",
       " 128: 'everyone',\n",
       " 129: 'evian',\n",
       " 130: 'example',\n",
       " 131: 'existed',\n",
       " 134: 'factoid',\n",
       " 135: 'family',\n",
       " 136: 'famous',\n",
       " 137: 'fancy',\n",
       " 138: 'fans',\n",
       " 144: 'flight',\n",
       " 145: 'follow',\n",
       " 146: 'for',\n",
       " 148: 'from',\n",
       " 151: 'gave',\n",
       " 153: 'getting',\n",
       " 156: 'giving',\n",
       " 158: 'gon',\n",
       " 162: 'great',\n",
       " 163: 'group',\n",
       " 164: 'grows',\n",
       " 165: 'had',\n",
       " 172: 'have',\n",
       " 173: 'he',\n",
       " 174: 'head',\n",
       " 175: 'heard',\n",
       " 176: 'hell',\n",
       " 178: 'higher-dimensional',\n",
       " 181: 'his',\n",
       " 183: 'how',\n",
       " 184: 'i',\n",
       " 186: 'if',\n",
       " 188: 'imagine',\n",
       " 189: 'impressive',\n",
       " 190: 'in',\n",
       " 194: 'inter-connecting',\n",
       " 196: 'interstellar',\n",
       " 198: 'is',\n",
       " 200: 'it',\n",
       " 201: 'itll',\n",
       " 202: 'its',\n",
       " 203: 'june',\n",
       " 204: 'just',\n",
       " 205: 'kardashian',\n",
       " 206: 'kart',\n",
       " 209: 'knew',\n",
       " 210: 'know',\n",
       " 214: 'life',\n",
       " 215: 'like',\n",
       " 216: 'limited',\n",
       " 217: 'literal',\n",
       " 222: 'man',\n",
       " 225: 'mario',\n",
       " 226: 'mates',\n",
       " 227: 'maybe',\n",
       " 228: 'me',\n",
       " 229: 'measure',\n",
       " 231: 'member',\n",
       " 232: 'message',\n",
       " 237: 'model',\n",
       " 239: 'more',\n",
       " 240: 'must',\n",
       " 241: 'my',\n",
       " 243: 'na',\n",
       " 246: 'next',\n",
       " 247: 'nice',\n",
       " 249: 'no',\n",
       " 252: 'not',\n",
       " 255: 'of',\n",
       " 258: 'okay',\n",
       " 262: 'one',\n",
       " 263: 'only',\n",
       " 264: 'opposite',\n",
       " 265: 'or',\n",
       " 267: 'out',\n",
       " 268: 'outside',\n",
       " 272: 'pads',\n",
       " 276: 'people',\n",
       " 280: 'phone',\n",
       " 281: 'pizza',\n",
       " 282: 'place',\n",
       " 283: 'placement',\n",
       " 289: 'police',\n",
       " 291: 'pooping',\n",
       " 293: 'pop',\n",
       " 294: 'potion',\n",
       " 297: 'pretending',\n",
       " 304: 're',\n",
       " 309: 'really',\n",
       " 310: 'reasoning',\n",
       " 312: 'rehab',\n",
       " 316: 'rice',\n",
       " 318: 'riding',\n",
       " 319: 'right',\n",
       " 320: 's',\n",
       " 321: 'same',\n",
       " 322: 'saved',\n",
       " 327: 'school',\n",
       " 330: 'sex',\n",
       " 332: 'shes',\n",
       " 340: 'snape',\n",
       " 341: 'snowden',\n",
       " 342: 'so',\n",
       " 345: 'someone',\n",
       " 346: 'something',\n",
       " 347: 'sometimes',\n",
       " 349: 'sorry',\n",
       " 350: 'speed',\n",
       " 351: 'spelled',\n",
       " 353: 'stall',\n",
       " 355: 'stick',\n",
       " 357: 'stock',\n",
       " 358: 'stomp',\n",
       " 359: 'stone',\n",
       " 361: 'story',\n",
       " 362: 'strict',\n",
       " 369: 't',\n",
       " 372: 'teacher',\n",
       " 373: 'technically',\n",
       " 378: 'than',\n",
       " 380: 'that',\n",
       " 381: 'thats',\n",
       " 382: 'the',\n",
       " 383: 'their',\n",
       " 384: 'them',\n",
       " 385: 'then',\n",
       " 389: 'they',\n",
       " 390: 'thing',\n",
       " 392: 'think',\n",
       " 394: 'this',\n",
       " 396: 'thought',\n",
       " 397: 'throws',\n",
       " 398: 'time',\n",
       " 400: 'to',\n",
       " 401: 'told',\n",
       " 405: 'tree',\n",
       " 408: 'truth',\n",
       " 412: 'universe',\n",
       " 415: 'up',\n",
       " 416: 'us',\n",
       " 421: 'vice',\n",
       " 424: 'walked',\n",
       " 425: 'walking',\n",
       " 428: 'wanted',\n",
       " 429: 'was',\n",
       " 430: 'wasnt',\n",
       " 433: 'wears',\n",
       " 436: 'went',\n",
       " 437: 'what',\n",
       " 438: 'when',\n",
       " 441: 'which',\n",
       " 443: 'white',\n",
       " 444: 'who',\n",
       " 446: 'wife',\n",
       " 447: 'will',\n",
       " 449: 'with',\n",
       " 451: 'worked',\n",
       " 452: 'world',\n",
       " 453: 'worn',\n",
       " 457: 'years',\n",
       " 459: 'you',\n",
       " 460: 'your',\n",
       " 462: '’'}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4, 5]\n",
    "b = [9, 8, 7, 6, 5]\n",
    "c = [1, 2, 3, 4, 5]\n",
    "a == c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len (word_indices.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len (list (word_vectors['hi']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(word_indices.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
