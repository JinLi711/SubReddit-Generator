{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "from keras.models import load_model\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import random\n",
    "# import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('RNN_weights.best.hdf5')\n",
    "model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Compute new probability distribution based on the temperature\n",
    "    Higher temperature creates more randomness.\n",
    "\n",
    "    :param preds: numpy array of shape (unique chars,), and elements sum to 1\n",
    "    :type  preds: numpy.ndarray\n",
    "    :param temperature: characterizes the entropy of probability distribution\n",
    "    :type  temperature: float\n",
    "    :returns: a number 0 to the length of preds - 1\n",
    "    :rtype:   int\n",
    "    \"\"\"\n",
    "\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_generate(model, text, word_indices, maxlen=10, temperature=1.0, textlen=40):\n",
    "    \"\"\"\n",
    "    Generate text based on a model.\n",
    "\n",
    "    :param model: trained keras model\n",
    "    :type  model: keras.engine.sequential.Sequential\n",
    "    :param text: lyrics\n",
    "    :type  text: str\n",
    "    :param char_indices: dictionary mapping a character to its integer placeholder\n",
    "    :type  char_indices: dict\n",
    "    :param maxlen: maximum length of the sequences\n",
    "    :type  maxlen: int\n",
    "    :param textlen: Number of characters of generated sequence\n",
    "    :type  textlen: int\n",
    "    \"\"\"\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated_text = text[start_index: start_index + maxlen]\n",
    "    full_sentence = \" \".join(generated_text)\n",
    "\n",
    "    out_text = generated_text\n",
    "\n",
    "    for i in range(textlen):\n",
    "\n",
    "        sampled = []\n",
    "        for t, word in enumerate(generated_text):\n",
    "            word_dimensions = list(wordvectors_mini[word])\n",
    "            sampled.append(word_dimensions)\n",
    "        sampled = np.array(sampled)\n",
    "        sampled = np.reshape(sampled, (1,) + sampled.shape)\n",
    "\n",
    "        preds = model.predict(sampled, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_index = str(next_index)\n",
    "        next_word = word_indices[next_index]\n",
    "\n",
    "        generated_text.append(next_word)\n",
    "        generated_text = generated_text[1:]\n",
    "        out_text.append(next_word)\n",
    "    return out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokens[:200]\n",
    "\n",
    "\n",
    "def find_random_sentence(tokens, word, maxlen):\n",
    "    list_of_appearance = np.where(np.array(tokens) == word)[0]\n",
    "    stop_characters = set({'...', '.', '?', '!'})\n",
    "    random_index = random.choice(list_of_appearance)\n",
    "    index = random_index\n",
    "\n",
    "    sentence = []\n",
    "    while (tokens[index] not in stop_characters):\n",
    "        sentence.append(tokens[index])\n",
    "        index += 1\n",
    "    sentence.append(tokens[index])\n",
    "\n",
    "    index = random_index\n",
    "\n",
    "    while ((tokens[index] not in stop_characters) or len(sentence) < 11):\n",
    "        sentence.insert(0, tokens[index])\n",
    "        index -= 1\n",
    "\n",
    "    return sentence[:maxlen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_generate_with_word(\n",
    "        model,\n",
    "        text,\n",
    "        word_indices,\n",
    "        word,\n",
    "        maxlen=10,\n",
    "        temperature=1.0,\n",
    "        textlen=40):\n",
    "    \"\"\"\n",
    "    Generate text based on a model.\n",
    "    The starting seed is based on a word input \n",
    "\n",
    "    :param model: trained keras model\n",
    "    :type  model: keras.engine.sequential.Sequential\n",
    "    :param text: lyrics\n",
    "    :type  text: str\n",
    "    :param char_indices: dictionary mapping a character to its integer placeholder\n",
    "    :type  char_indices: dict\n",
    "    :param word: the input starting word\n",
    "    :type  word: str\n",
    "    :param maxlen: maximum length of the sequences\n",
    "    :type  maxlen: int\n",
    "    :param textlen: Number of characters of generated sequence\n",
    "    :type  textlen: int\n",
    "    \"\"\"\n",
    "\n",
    "    stop_characters = set({'...', '.', '?', '!'})\n",
    "\n",
    "    generated_text = find_random_sentence(tokens, word, maxlen)\n",
    "    full_sentence = \" \".join(generated_text)\n",
    "\n",
    "    out_text = generated_text\n",
    "\n",
    "#     for i in range(textlen):\n",
    "    stop_generate = False\n",
    "    i = 0\n",
    "    while ((i < textlen) or (not stop_generate)):\n",
    "\n",
    "        sampled = []\n",
    "        for t, word in enumerate(generated_text):\n",
    "            word_dimensions = list(wordvectors_mini[word])\n",
    "            sampled.append(word_dimensions)\n",
    "        sampled = np.array(sampled)\n",
    "        sampled = np.reshape(sampled, (1,) + sampled.shape)\n",
    "\n",
    "        preds = model.predict(sampled, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_index = str(next_index)\n",
    "        next_word = word_indices[next_index]\n",
    "\n",
    "        generated_text.append(next_word)\n",
    "        generated_text = generated_text[1:]\n",
    "        out_text.append(next_word)\n",
    "\n",
    "        if (next_word in stop_characters):\n",
    "            stop_generate = True\n",
    "        i += 1\n",
    "    return out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this so I download the text after making this changes, so I can just upload th\n",
    "# updated text\n",
    "# actually, change it so \n",
    "import re\n",
    "text = open('all.txt', 'r').read()\n",
    "text = text.lower()\n",
    "text = re.sub(r'[*^$%&()@#-+_=//]', ' ', text)\n",
    "text = re.sub(\" \\d+\", \"number\", text)\n",
    "text = re.sub(r'http\\S+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)\n",
    "with open('mapping.json') as infile:\n",
    "    word_indices = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens\n",
    "# word_indices\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "# tokens2 = Tokenizer\n",
    "\n",
    "tokens2 = text_to_word_sequence(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['did',\n",
       " 'you',\n",
       " 'have',\n",
       " 'your',\n",
       " 'showerhead',\n",
       " 'plugged',\n",
       " 'in',\n",
       " 'or',\n",
       " 'is',\n",
       " 'it',\n",
       " 'a',\n",
       " 'wireless',\n",
       " 'model',\n",
       " 'yep',\n",
       " 'and',\n",
       " 'when',\n",
       " 'you',\n",
       " 'finally',\n",
       " 'switch',\n",
       " 'to',\n",
       " 'another',\n",
       " 'career',\n",
       " 'it',\n",
       " 'really',\n",
       " 'throws',\n",
       " 'you',\n",
       " 'off',\n",
       " 'when',\n",
       " 'your',\n",
       " 'coworkers',\n",
       " 'measure',\n",
       " 'years',\n",
       " 'differently',\n",
       " 'thats',\n",
       " 'a',\n",
       " 'nice',\n",
       " 'dark',\n",
       " 'thought',\n",
       " 'that',\n",
       " 'will',\n",
       " 'now',\n",
       " 'pop',\n",
       " 'into',\n",
       " 'my',\n",
       " 'head',\n",
       " 'during',\n",
       " 'my',\n",
       " 'next',\n",
       " 'birthday',\n",
       " 'i',\n",
       " 'want',\n",
       " 'to',\n",
       " 'be',\n",
       " 'cremated',\n",
       " 'and',\n",
       " 'then',\n",
       " 'put',\n",
       " 'into',\n",
       " 'one',\n",
       " 'of',\n",
       " 'those',\n",
       " 'cardboard',\n",
       " 'pods',\n",
       " 'that',\n",
       " 'grows',\n",
       " 'into',\n",
       " 'a',\n",
       " 'tree',\n",
       " 'so',\n",
       " 'one',\n",
       " 'day',\n",
       " 'i',\n",
       " 'will',\n",
       " 'be',\n",
       " 'cut',\n",
       " 'down',\n",
       " 'and',\n",
       " 'turned',\n",
       " 'into',\n",
       " 'someone',\n",
       " 'elses',\n",
       " 'coffin',\n",
       " 'would',\n",
       " 'they',\n",
       " 'think',\n",
       " 'it’s',\n",
       " 'cool',\n",
       " 'if',\n",
       " 'it',\n",
       " 'was',\n",
       " 'vomit',\n",
       " 'u',\n",
       " 'the',\n",
       " 'only',\n",
       " 'one',\n",
       " 'thinking',\n",
       " 'that',\n",
       " 'bro',\n",
       " 'i',\n",
       " 'wanted',\n",
       " 'to',\n",
       " 'get',\n",
       " 'to',\n",
       " 'know',\n",
       " 'somebody',\n",
       " 'better',\n",
       " 'so',\n",
       " 'i',\n",
       " 'asked',\n",
       " 'them',\n",
       " 'how',\n",
       " 'their',\n",
       " 'vaction',\n",
       " 'went',\n",
       " 'in',\n",
       " 'mexico',\n",
       " 'simple',\n",
       " 'enough',\n",
       " 'right',\n",
       " 'she',\n",
       " 'told',\n",
       " 'me',\n",
       " 'to',\n",
       " 'follow',\n",
       " 'her',\n",
       " 'instagram',\n",
       " 'where',\n",
       " 'i',\n",
       " 'could',\n",
       " 'find',\n",
       " 'out',\n",
       " 'for',\n",
       " 'myself',\n",
       " 'at',\n",
       " 'least',\n",
       " 'it',\n",
       " 'saved',\n",
       " 'me',\n",
       " 'the',\n",
       " 'effort',\n",
       " 'of',\n",
       " 'getting',\n",
       " 'to',\n",
       " 'know',\n",
       " 'them',\n",
       " 'in',\n",
       " 'the',\n",
       " 'first',\n",
       " 'place',\n",
       " 'edit',\n",
       " 'i',\n",
       " 'know',\n",
       " 'she',\n",
       " 'probably',\n",
       " 'wasnt',\n",
       " 'interested',\n",
       " 'in',\n",
       " 'getting',\n",
       " 'to',\n",
       " 'know',\n",
       " 'me',\n",
       " 'but',\n",
       " 'a',\n",
       " 'simple',\n",
       " 'it',\n",
       " 'was',\n",
       " 'good',\n",
       " 'would',\n",
       " 'have',\n",
       " 'gotten',\n",
       " 'the',\n",
       " 'message',\n",
       " 'across',\n",
       " 'just',\n",
       " 'as',\n",
       " 'well',\n",
       " 'its',\n",
       " 'just',\n",
       " 'the',\n",
       " 'outside',\n",
       " 'catching',\n",
       " 'up',\n",
       " 'with',\n",
       " 'the',\n",
       " 'inside',\n",
       " 'i',\n",
       " 'am',\n",
       " 'sorry',\n",
       " 'detective',\n",
       " 'my',\n",
       " 'responses',\n",
       " 'are',\n",
       " 'limited',\n",
       " 'you',\n",
       " 'must',\n",
       " 'ask',\n",
       " 'the',\n",
       " 'right',\n",
       " 'questions',\n",
       " 'what',\n",
       " 'about',\n",
       " 'the',\n",
       " 'cheese',\n",
       " 'and',\n",
       " 'gravy',\n",
       " 'evian',\n",
       " 'is',\n",
       " 'naive',\n",
       " 'spelled',\n",
       " 'backwards',\n",
       " 'i',\n",
       " 'think',\n",
       " 'it',\n",
       " 'would',\n",
       " 'be',\n",
       " 'scarier',\n",
       " 'if',\n",
       " 'it',\n",
       " 'just',\n",
       " 'randomly',\n",
       " 'gave',\n",
       " 'you',\n",
       " 'a',\n",
       " 'factoid',\n",
       " 'about',\n",
       " 'you',\n",
       " 'based',\n",
       " 'on',\n",
       " 'what',\n",
       " 'it',\n",
       " 'knows',\n",
       " 'pretending',\n",
       " 'it',\n",
       " 'didnt',\n",
       " 'know',\n",
       " 'these',\n",
       " 'things',\n",
       " 'but',\n",
       " 'slowly',\n",
       " 'making',\n",
       " 'you',\n",
       " 'aware',\n",
       " 'that',\n",
       " 'it',\n",
       " 'has',\n",
       " 'mapped',\n",
       " 'your',\n",
       " 'entire',\n",
       " 'life',\n",
       " 'no',\n",
       " 'bro',\n",
       " 'is',\n",
       " 'oil',\n",
       " 'what',\n",
       " 'if',\n",
       " 'the',\n",
       " 'aliens',\n",
       " 'doing',\n",
       " 'that',\n",
       " 'to',\n",
       " 'us',\n",
       " 'are',\n",
       " 'only',\n",
       " 'doing',\n",
       " 'it',\n",
       " 'because',\n",
       " 'someone',\n",
       " 'did',\n",
       " 'it',\n",
       " 'to',\n",
       " 'them',\n",
       " 'first',\n",
       " 'perhaps',\n",
       " 'its',\n",
       " 'the',\n",
       " 'universes',\n",
       " 'oldest',\n",
       " 'interstellar',\n",
       " 'prank',\n",
       " 'the',\n",
       " 'only',\n",
       " 'other',\n",
       " 'white',\n",
       " 'person',\n",
       " 'who',\n",
       " 'could',\n",
       " 'get',\n",
       " 'away',\n",
       " 'with',\n",
       " 'it',\n",
       " 'is',\n",
       " 'danny',\n",
       " 'devito',\n",
       " 'same',\n",
       " 'if',\n",
       " 'you',\n",
       " 'stomp',\n",
       " 'on',\n",
       " 'some',\n",
       " 'rice',\n",
       " 'crispies',\n",
       " 'technically',\n",
       " 'youre',\n",
       " 'giving',\n",
       " 'them',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'week',\n",
       " 'or',\n",
       " 'it',\n",
       " 'feels',\n",
       " 'like',\n",
       " 'week',\n",
       " 'for',\n",
       " 'them',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'example',\n",
       " 'of',\n",
       " 'over',\n",
       " 'thinking',\n",
       " 'it',\n",
       " 'give',\n",
       " 'fans',\n",
       " 'long',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'obsess',\n",
       " 'over',\n",
       " 'a',\n",
       " 'story',\n",
       " 'and',\n",
       " 'they',\n",
       " 'will',\n",
       " 'come',\n",
       " 'up',\n",
       " 'with',\n",
       " 'theories',\n",
       " 'inter',\n",
       " 'connecting',\n",
       " 'every',\n",
       " 'little',\n",
       " 'thing',\n",
       " 'truth',\n",
       " 'is',\n",
       " 'hagrid',\n",
       " 'and',\n",
       " 'snape',\n",
       " 'wouldnt',\n",
       " 'get',\n",
       " 'along',\n",
       " 'whether',\n",
       " 'harry',\n",
       " 'existed',\n",
       " 'or',\n",
       " 'not',\n",
       " 'plus',\n",
       " 'its',\n",
       " 'clear',\n",
       " 'during',\n",
       " 'sorcerors',\n",
       " 'stone',\n",
       " 'that',\n",
       " 'they',\n",
       " 'already',\n",
       " 'dont',\n",
       " 'get',\n",
       " 'along',\n",
       " 'for',\n",
       " 'many',\n",
       " 'years',\n",
       " 'beforehand',\n",
       " 'someone',\n",
       " 'at',\n",
       " 'a',\n",
       " 'stock',\n",
       " 'brokerage',\n",
       " 'i',\n",
       " 'worked',\n",
       " 'at',\n",
       " 'would',\n",
       " 'stick',\n",
       " 'her',\n",
       " 'dirty',\n",
       " 'pads',\n",
       " 'to',\n",
       " 'the',\n",
       " 'stall',\n",
       " 'wall',\n",
       " 'all',\n",
       " 'the',\n",
       " 'time',\n",
       " 'it',\n",
       " 'disturbed',\n",
       " 'me',\n",
       " 'that',\n",
       " 'someone',\n",
       " 'i',\n",
       " 'knew',\n",
       " 'and',\n",
       " 'worked',\n",
       " 'with',\n",
       " 'was',\n",
       " 'so',\n",
       " 'deliberately',\n",
       " 'disgusting',\n",
       " 'not',\n",
       " 'really',\n",
       " 'people',\n",
       " 'starving',\n",
       " 'in',\n",
       " 'africa',\n",
       " 'would',\n",
       " 'be',\n",
       " 'even',\n",
       " 'more',\n",
       " 'fucked',\n",
       " 'emphysema',\n",
       " 'isnt',\n",
       " 'something',\n",
       " 'you',\n",
       " 'would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'have',\n",
       " 'and',\n",
       " 'be',\n",
       " 'the',\n",
       " 'only',\n",
       " 'white',\n",
       " 'girl',\n",
       " 'that',\n",
       " 'can',\n",
       " 'thank',\n",
       " 'you',\n",
       " 'for',\n",
       " 'making',\n",
       " 'sure',\n",
       " 'i',\n",
       " 'think',\n",
       " 'about',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'shes',\n",
       " 'a',\n",
       " 'family',\n",
       " 'member',\n",
       " 'the',\n",
       " 'next',\n",
       " 'time',\n",
       " 'i',\n",
       " 'have',\n",
       " 'sex',\n",
       " 'with',\n",
       " 'my',\n",
       " 'wife',\n",
       " 'i',\n",
       " 'really',\n",
       " 'appreciate',\n",
       " 'that',\n",
       " 'the',\n",
       " 'other',\n",
       " 'day',\n",
       " 'i',\n",
       " 'walked',\n",
       " 'by',\n",
       " 'a',\n",
       " 'man',\n",
       " 'looking',\n",
       " 'at',\n",
       " 'his',\n",
       " 'phone',\n",
       " 'while',\n",
       " 'his',\n",
       " 'dog',\n",
       " 'walked',\n",
       " 'along',\n",
       " 'leaving',\n",
       " 'a',\n",
       " 'little',\n",
       " 'trail',\n",
       " 'of',\n",
       " 'poos',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'maybe',\n",
       " 'he',\n",
       " 'didnt',\n",
       " 'realize',\n",
       " 'his',\n",
       " 'dog',\n",
       " 'was',\n",
       " 'pooping',\n",
       " 'since',\n",
       " 'he',\n",
       " 'was',\n",
       " 'paying',\n",
       " 'attention',\n",
       " 'to',\n",
       " 'his',\n",
       " 'phone',\n",
       " 'and',\n",
       " 'his',\n",
       " 'dog',\n",
       " 'hadnt',\n",
       " 'stopped',\n",
       " 'walking',\n",
       " 'he',\n",
       " 'did',\n",
       " 'not',\n",
       " 'appreciate',\n",
       " 'me',\n",
       " 'pointing',\n",
       " 'this',\n",
       " 'out',\n",
       " 'to',\n",
       " 'him',\n",
       " 'and',\n",
       " 'their',\n",
       " 'rehab',\n",
       " 'just',\n",
       " 'ends',\n",
       " 'up',\n",
       " 'being',\n",
       " 'the',\n",
       " 'opposite',\n",
       " 'of',\n",
       " 'rehab',\n",
       " 'who',\n",
       " 'knows',\n",
       " 'maybe',\n",
       " 'its',\n",
       " 'just',\n",
       " 'higher',\n",
       " 'dimensional',\n",
       " 'advertising',\n",
       " 'billboards',\n",
       " 'the',\n",
       " 'best',\n",
       " 'reasoning',\n",
       " 'for',\n",
       " 'something',\n",
       " 'like',\n",
       " 'this',\n",
       " 'i',\n",
       " 'heard',\n",
       " 'is',\n",
       " 'that',\n",
       " 'fancy',\n",
       " 'is',\n",
       " 'just',\n",
       " 'the',\n",
       " 'opposite',\n",
       " 'of',\n",
       " 'how',\n",
       " 'its',\n",
       " 'normally',\n",
       " 'worn',\n",
       " 'i',\n",
       " 'e',\n",
       " 'someone',\n",
       " 'who',\n",
       " 'wears',\n",
       " 'their',\n",
       " 'hair',\n",
       " 'down',\n",
       " 'would',\n",
       " 'put',\n",
       " 'it',\n",
       " 'up',\n",
       " 'and',\n",
       " 'vice',\n",
       " 'versa',\n",
       " 'not',\n",
       " 'a',\n",
       " 'universal',\n",
       " 'truth',\n",
       " 'mind',\n",
       " 'does',\n",
       " 'duckduckgo',\n",
       " 'ever',\n",
       " 'get',\n",
       " 'product',\n",
       " 'placement',\n",
       " 'haha',\n",
       " 'great',\n",
       " 'insight',\n",
       " 'thats',\n",
       " 'hot',\n",
       " 'as',\n",
       " 'hell',\n",
       " 'okay',\n",
       " 'that’s',\n",
       " 'mind',\n",
       " 'blowing',\n",
       " 'riding',\n",
       " 'a',\n",
       " 'ten',\n",
       " 'speed',\n",
       " 'down',\n",
       " 'a',\n",
       " 'flight',\n",
       " 'of',\n",
       " 'stairs',\n",
       " 'or',\n",
       " 'naming',\n",
       " 'their',\n",
       " 'kids',\n",
       " 'boy',\n",
       " 'and',\n",
       " 'girl',\n",
       " 'and',\n",
       " 'then',\n",
       " 'they',\n",
       " 'see',\n",
       " 'us',\n",
       " 'and',\n",
       " 'say',\n",
       " 'the',\n",
       " 'test',\n",
       " 'subjects',\n",
       " 'are',\n",
       " 'still',\n",
       " 'alive',\n",
       " '88',\n",
       " 'is',\n",
       " 'justnumber',\n",
       " 'for',\n",
       " 'chubby',\n",
       " 'people',\n",
       " 'i',\n",
       " 'could',\n",
       " 'not',\n",
       " 'imagine',\n",
       " 'going',\n",
       " 'to',\n",
       " 'school',\n",
       " 'for',\n",
       " 'your',\n",
       " 'entire',\n",
       " 'life',\n",
       " 'which',\n",
       " 'is',\n",
       " 'part',\n",
       " 'of',\n",
       " 'why',\n",
       " 'i',\n",
       " 'am',\n",
       " 'not',\n",
       " 'gonna',\n",
       " 'be',\n",
       " 'a',\n",
       " 'teacher',\n",
       " 'sometimes',\n",
       " 'thats',\n",
       " 'way',\n",
       " 'too',\n",
       " 'literal',\n",
       " 'like',\n",
       " 'in',\n",
       " 'brazil',\n",
       " 'where',\n",
       " 'police',\n",
       " 'and',\n",
       " 'criminal',\n",
       " 'group',\n",
       " 'corruption',\n",
       " 'can',\n",
       " 'sometimes',\n",
       " 'overlap',\n",
       " 'with',\n",
       " 'eachother',\n",
       " 'there',\n",
       " 'are',\n",
       " 'more',\n",
       " 'non',\n",
       " 'virgins',\n",
       " 'than',\n",
       " 'virgins',\n",
       " 'my',\n",
       " 'daughters',\n",
       " 'class',\n",
       " 'hasnumber',\n",
       " '000',\n",
       " 'students',\n",
       " 'in',\n",
       " 'it',\n",
       " 'and',\n",
       " 'my',\n",
       " 'wife',\n",
       " 'says',\n",
       " 'no',\n",
       " 'flask',\n",
       " 'junenumber',\n",
       " 'is',\n",
       " 'gonna',\n",
       " 'suck',\n",
       " 'edit',\n",
       " 'was',\n",
       " 'not',\n",
       " 'expecting',\n",
       " 'this',\n",
       " 'response',\n",
       " 'but',\n",
       " 'i',\n",
       " 'have',\n",
       " 'had',\n",
       " 'a',\n",
       " 'fun',\n",
       " 'time',\n",
       " 'reading',\n",
       " 'all',\n",
       " 'of',\n",
       " 'your',\n",
       " 'responses',\n",
       " 'with',\n",
       " 'my',\n",
       " 'wife',\n",
       " 'and',\n",
       " 'daughter',\n",
       " 'who',\n",
       " 'have',\n",
       " 'found',\n",
       " 'them',\n",
       " 'hilarious',\n",
       " 'family',\n",
       " 'fun',\n",
       " 'night',\n",
       " 'number',\n",
       " 'version',\n",
       " 'i',\n",
       " 'think',\n",
       " 'itll',\n",
       " 'be',\n",
       " 'more',\n",
       " 'impressive',\n",
       " 'because',\n",
       " 'number',\n",
       " 'accidents',\n",
       " 'will',\n",
       " 'be',\n",
       " 'rare',\n",
       " 'number',\n",
       " 'automated',\n",
       " 'car',\n",
       " 'accidents',\n",
       " 'even',\n",
       " 'more',\n",
       " 'rare',\n",
       " 'drive',\n",
       " 'throughs',\n",
       " 'that',\n",
       " 'won’t',\n",
       " 'serve',\n",
       " 'pedestrians',\n",
       " 'are',\n",
       " 'basically',\n",
       " 'saying',\n",
       " 'they',\n",
       " 'have',\n",
       " 'a',\n",
       " 'strict',\n",
       " 'dress',\n",
       " 'code',\n",
       " 'that',\n",
       " 'requires',\n",
       " 'you',\n",
       " 'to',\n",
       " 'wear',\n",
       " 'a',\n",
       " 'car',\n",
       " 'in',\n",
       " 'the',\n",
       " 'harry',\n",
       " 'potter',\n",
       " 'universe',\n",
       " 'couples',\n",
       " 'could',\n",
       " 'take',\n",
       " 'polyjuice',\n",
       " 'potion',\n",
       " 'and',\n",
       " 'swap',\n",
       " 'bodies',\n",
       " 'then',\n",
       " 'have',\n",
       " 'sex',\n",
       " 'to',\n",
       " 'see',\n",
       " 'what',\n",
       " 'its',\n",
       " 'like',\n",
       " 'from',\n",
       " 'both',\n",
       " 'perspectives',\n",
       " 'this',\n",
       " 'might',\n",
       " 'be',\n",
       " 'a',\n",
       " 'record',\n",
       " 'for',\n",
       " 'worst',\n",
       " 'shower',\n",
       " 'thought',\n",
       " 'my',\n",
       " 'mates',\n",
       " 'already',\n",
       " 'like',\n",
       " 'that',\n",
       " 'apparently',\n",
       " 'a',\n",
       " 'topnumber',\n",
       " 'world',\n",
       " 'champion',\n",
       " 'at',\n",
       " 'mario',\n",
       " 'kart',\n",
       " 'so',\n",
       " 'im',\n",
       " 'sure',\n",
       " 'when',\n",
       " 'his',\n",
       " 'kids',\n",
       " 'are',\n",
       " 'playing',\n",
       " 'mario',\n",
       " 'kartnumber',\n",
       " 'hell',\n",
       " 'be',\n",
       " 'telling',\n",
       " 'that',\n",
       " 'same',\n",
       " 'ol',\n",
       " 'story',\n",
       " 'real',\n",
       " 'life',\n",
       " 'is',\n",
       " 'likenumber',\n",
       " 'diplomacy',\n",
       " 'checks',\n",
       " 'smokinnumber',\n",
       " 'while',\n",
       " 'readin',\n",
       " 'this',\n",
       " 'that',\n",
       " 'cool',\n",
       " 'a',\n",
       " 'pizza',\n",
       " 'is',\n",
       " 'just',\n",
       " 'a',\n",
       " 'modern',\n",
       " 'version',\n",
       " 'of',\n",
       " 'a',\n",
       " 'trencher',\n",
       " 'from',\n",
       " 'medieval',\n",
       " 'times',\n",
       " 'and',\n",
       " 'edward',\n",
       " 'snowden',\n",
       " 'should',\n",
       " 'do',\n",
       " 'it',\n",
       " 'the',\n",
       " 'kardashian’s',\n",
       " 'can’t',\n",
       " 'be',\n",
       " 'that',\n",
       " 'dumb',\n",
       " 'they’re',\n",
       " 'rich',\n",
       " 'and',\n",
       " 'famous',\n",
       " 'and',\n",
       " 'tricked',\n",
       " 'everyone',\n",
       " 'into',\n",
       " 'caring',\n",
       " 'about',\n",
       " 'them',\n",
       " 'i',\n",
       " 'tell',\n",
       " 'people',\n",
       " 'not',\n",
       " 'to',\n",
       " 'talk',\n",
       " 'to',\n",
       " 'me',\n",
       " 'until',\n",
       " 'i’ve',\n",
       " 'had',\n",
       " 'a',\n",
       " 'coffee',\n",
       " 'the',\n",
       " 'key',\n",
       " 'is',\n",
       " 'not',\n",
       " 'to',\n",
       " 'drink',\n",
       " 'coffee',\n",
       " 'the',\n",
       " 'better',\n",
       " 'you',\n",
       " 'are',\n",
       " 'at',\n",
       " 'doing',\n",
       " 'something',\n",
       " 'the',\n",
       " 'more',\n",
       " 'eccentric',\n",
       " 'you',\n",
       " 'can',\n",
       " 'dress',\n",
       " 'while',\n",
       " 'doing',\n",
       " 'it',\n",
       " 'the',\n",
       " 'first',\n",
       " 'time',\n",
       " 'someone',\n",
       " 'steals',\n",
       " 'a',\n",
       " 'spaceship',\n",
       " 'and',\n",
       " 'flies',\n",
       " 'away',\n",
       " 'is',\n",
       " 'going',\n",
       " 'to',\n",
       " 'be',\n",
       " 'epic',\n",
       " 'you',\n",
       " 'were',\n",
       " 'pulled',\n",
       " 'off',\n",
       " 'by',\n",
       " 'your',\n",
       " 'hammer',\n",
       " 'it’s',\n",
       " 'funny',\n",
       " 'that',\n",
       " 'we',\n",
       " 'still',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'the',\n",
       " 'pyramids',\n",
       " 'as',\n",
       " 'one',\n",
       " 'of',\n",
       " 'thenumber',\n",
       " 'wonders',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " 'when',\n",
       " 'they’re',\n",
       " 'the',\n",
       " 'only',\n",
       " 'ones',\n",
       " 'left',\n",
       " 'theyve',\n",
       " 'really',\n",
       " 'isolated',\n",
       " 'themselves',\n",
       " 'from',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'competition',\n",
       " 'dave',\n",
       " 'i',\n",
       " 'dont',\n",
       " 'have',\n",
       " 'to',\n",
       " 'go',\n",
       " 'to',\n",
       " 'walmart',\n",
       " 'to',\n",
       " 'be',\n",
       " 'the',\n",
       " 'weird',\n",
       " 'person',\n",
       " 'since',\n",
       " 'kinetic',\n",
       " 'energy',\n",
       " 'is',\n",
       " 'converted',\n",
       " 'into',\n",
       " 'thermal',\n",
       " 'energy',\n",
       " 'slapping',\n",
       " 'an',\n",
       " 'uncooked',\n",
       " ...]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordvectors_mini = json.loads('wordvectors_mini.json')#.decode(\"utf-8\")\n",
    "# wordvectors_mini = json.load('test.json')\n",
    "with open('wordvectors_mini.json') as infile:\n",
    "    wordvectors_mini = json.load(infile)\n",
    "# text_generate_with_word(model, tokens, word_indices, 'pizza')\n",
    "# test_csv = pd.read_csv('wordvectors_mini.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_csv.to_dict()#.shape#.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_csv\n",
    "# wordvectors_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_text():\n",
    "    \n",
    "    import language_check\n",
    "    tool = language_check.LanguageTool('en-US')\n",
    "    \n",
    "    try: \n",
    "#         sentence = text_generate_with_word(\n",
    "#             model, tokens, word_indices, 'pizza', maxlen=20\n",
    "#         )\n",
    "        sentence = text_generate(model, tokens, word_indices, maxlen=20)\n",
    "        final_text = ' '.join(sentence)\n",
    "        matches = tool.check(final_text)\n",
    "        final_text = language_check.correct(final_text, matches)\n",
    "        return final_text\n",
    "    except ValueError:\n",
    "        sentence = text_generate(model, tokens, word_indices, maxlen=20)\n",
    "        final_text = ' '.join (sentence)\n",
    "        matches = tool.check(final_text)\n",
    "        final_text = language_check.correct(final_text, matches)\n",
    "        return final_text\n",
    "    except KeyError:\n",
    "        gen_text()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file(n):\n",
    "    file = open('sample_text.txt', 'a')\n",
    "    for i in range(n):\n",
    "        try:\n",
    "            text = gen_text()\n",
    "            if text != None:\n",
    "                file.write(text)\n",
    "                file.write('\\n')\n",
    "        except KeyError:\n",
    "            pass\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it was either this or no nose. ” perhaps the reason pizza pizza is so good is because its the friends friends was a think you dont between bad s ’ from the by drug yet happy , but faster like i have to kinda i just that a one bad how my dad . im they we have nothing to'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_text = ' '.join (text_generate(model, tokens, word_indices, maxlen=20))\n",
    "final_text = gen_text()\n",
    "final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordvectors_mini['bynumber']#.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It was either this or no nose. ” perhaps the reason pizza is so good is because it's the friends was a think you Mont between bad s ’ from the by drug yet happy, but faster like i have to kinda i just that a one bad how my dad. Am they we have nothing to\""
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import language_check\n",
    "tool = language_check.LanguageTool('en-US')\n",
    "matches = tool.check(final_text)\n",
    "language_check.correct(final_text, matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The work. Caveman comedian : so a guy and bear walk into bar. The bartender says, what ll)) now they for a goes ! * - fire 1 : explain out that ” this can look it asked putting will go in the to be and gave it. there ’ s what about that day'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import random\n",
    "\n",
    "def read_random_line(directory):\n",
    "    \n",
    "    lines = open(directory).read().splitlines()\n",
    "    myline = random.choice(lines)\n",
    "    return(myline)\n",
    "    \n",
    "read_random_line('sample_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
